[
  {
    "objectID": "notebooks/sql_qs.html",
    "href": "notebooks/sql_qs.html",
    "title": "Exploring a database with SQL",
    "section": "",
    "text": "Code\nimport sqlite3\nimport pandas as pd\nfrom IPython.display import Markdown, display\n\nfrom src.util import DBManager"
  },
  {
    "objectID": "notebooks/sql_qs.html#imports",
    "href": "notebooks/sql_qs.html#imports",
    "title": "Exploring a database with SQL",
    "section": "",
    "text": "Code\nimport sqlite3\nimport pandas as pd\nfrom IPython.display import Markdown, display\n\nfrom src.util import DBManager"
  },
  {
    "objectID": "notebooks/sql_qs.html#define-the-tables",
    "href": "notebooks/sql_qs.html#define-the-tables",
    "title": "Exploring a database with SQL",
    "section": "Define the tables",
    "text": "Define the tables\nLet us imagine we have the following tables in our database:\n\nSales\n\n\nsales_id (INT)\ncustomer_id (INT)\nproduct_id (INT)\nsale_date (DATE)\nquantity (INT)\ntotal_amount (DECIMAL)\n\n\nCustomers\n\n\ncustomer_id (INT)\ncustomer_name (VARCHAR)\nsales_region (VARCHAR)\nsign_up_date (DATE)\n\n\nProducts\n\n\nproduct_id (INT)\nproduct_name (VARCHAR)\ncategory (VARCHAR)\nprice (DECIMAL)\n\nWe can simulate this scenario by creating a test database in Python, creating tables within it that match this description, and inserting some example values into the tables.\n\n\nCreate a database\ndb_name = \"testdatabase.db\"\ndb = DBManager(db_name)\nconn, cursor = db.open()\n\n\n\n\nCreate the tables\n# Create `Sales` table\ncursor.execute(\n    \"\"\"\n    CREATE TABLE IF NOT EXISTS Sales\n    (\n        sales_id        INTEGER PRIMARY KEY AUTOINCREMENT,\n        customer_id     INTEGER,\n        product_id      INTEGER,\n        sales_date      DATE,\n        quantity        INTEGER,\n        total_amount    DECIMAL(10, 2),\n        FOREIGN KEY (customer_id) REFERENCES Customers(customer_id),\n        FOREIGN KEY (product_id) REFERENCES Products(product_id)\n    );\n    \"\"\"\n)\n\n# Create `Customers` table\ncursor.execute(\n    \"\"\"\n    CREATE TABLE IF NOT EXISTS Customers\n    (\n        customer_id     INTEGER PRIMARY KEY AUTOINCREMENT,\n        customer_name   VARCHAR(255) NOT NULL,\n        sales_region    VARCHAR(255),\n        sign_up_date    DATE\n    );\n    \"\"\"\n)\n\n# Create `Products` table\ncursor.execute(\n    \"\"\"\n    CREATE TABLE IF NOT EXISTS Products\n    (\n        product_id      INTEGER PRIMARY KEY AUTOINCREMENT,\n        product_name    VARCHAR(255) NOT NULL,\n        category        VARCHAR(255),\n        price           DECIMAL(10, 2)\n    );\n    \"\"\"\n)\n\n\n&lt;sqlite3.Cursor at 0x1178efa40&gt;\n\n\n\n\nInsert example data into the tables\n# Insert record into `Customers` table\nquery = \"\"\"\n    INSERT INTO Customers\n    (customer_name, sales_region, sign_up_date)\n    VALUES (?, ?, ?);\n\"\"\"\nvalues = [\n    (\"John Doe\", \"West\", \"2023-09-25\"),\n    (\"Jane Young\", \"South\", \"2024-09-25\"),\n    (\"Chris Nguyen\", \"West\", \"2024-09-25\"),\n]\ncursor.executemany(query, values)\n\n# Insert record into `Products` table\nquery = \"\"\"\n    INSERT INTO Products\n    (product_name, category, price)\n    VALUES (?, ?, ?);\n\"\"\"\nvalues = [\n    (\"Washing machine\", \"Appliances\", 1500.00),\n    (\"Laptop\", \"Electronics\", 1000.00),\n    (\"Phone\", \"Electronics\", 800.00),\n]\ncursor.executemany(query, values)\n\n# Insert record into `Sales` table\nquery = \"\"\"\n    INSERT INTO Sales\n    (customer_id, product_id, sales_date, quantity, total_amount)\n    VALUES (?, ?, ?, ?, ?);\n\"\"\"\nvalues = [\n    (1, 1, \"2023-09-26\", 2, values[0][2] * 2),\n    (2, 1, \"2023-01-15\", 4, values[0][2] * 4),\n    (2, 2, \"2024-09-20\", 3, values[1][2] * 3),\n    (3, 3, \"2024-08-22\", 9, values[2][2] * 10),\n    (1, 2, \"2023-09-26\", 40, values[1][2] * 40),\n]\ncursor.executemany(query, values)\n\n\n&lt;sqlite3.Cursor at 0x1178efa40&gt;\n\n\n\n\nCheck the contents of the tables\n# Query `Sales`\nquery = \"\"\"\n    SELECT *\n    FROM Sales\n    LIMIT 5;\n\"\"\"\ndisplay(Markdown(\"**`Sales`**:\"), pd.read_sql(query, conn))\n\n# Query `Customers`\nquery = \"\"\"\n    SELECT *\n    FROM Customers\n    LIMIT 5;\n\"\"\"\ndisplay(Markdown(\"**`Customers`**:\"), pd.read_sql(query, conn))\n\n# Query `Products`\nquery = \"\"\"\n    SELECT *\n    FROM Products\n    LIMIT 5;\n\"\"\"\ndisplay(Markdown(\"**`Products`**:\"), pd.read_sql(query, conn))\n\n\nSales:\n\n\n\n\n\n  \n    \n      \n      sales_id\n      customer_id\n      product_id\n      sales_date\n      quantity\n      total_amount\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2023-09-26\n      2\n      3000\n    \n    \n      1\n      2\n      2\n      1\n      2023-01-15\n      4\n      6000\n    \n    \n      2\n      3\n      2\n      2\n      2024-09-20\n      3\n      3000\n    \n    \n      3\n      4\n      3\n      3\n      2024-08-22\n      9\n      8000\n    \n    \n      4\n      5\n      1\n      2\n      2023-09-26\n      40\n      40000\n    \n  \n\n\n\n\nCustomers:\n\n\n\n\n\n  \n    \n      \n      customer_id\n      customer_name\n      sales_region\n      sign_up_date\n    \n  \n  \n    \n      0\n      1\n      John Doe\n      West\n      2023-09-25\n    \n    \n      1\n      2\n      Jane Young\n      South\n      2024-09-25\n    \n    \n      2\n      3\n      Chris Nguyen\n      West\n      2024-09-25\n    \n  \n\n\n\n\nProducts:\n\n\n\n\n\n  \n    \n      \n      product_id\n      product_name\n      category\n      price\n    \n  \n  \n    \n      0\n      1\n      Washing machine\n      Appliances\n      1500\n    \n    \n      1\n      2\n      Laptop\n      Electronics\n      1000\n    \n    \n      2\n      3\n      Phone\n      Electronics\n      800"
  },
  {
    "objectID": "notebooks/sql_qs.html#query-the-tables",
    "href": "notebooks/sql_qs.html#query-the-tables",
    "title": "Exploring a database with SQL",
    "section": "Query the tables",
    "text": "Query the tables\nWith these example tables constructed, let us now run some queries.\n\nQ1\nWrite a query to return the customer_name, product_name, and total_amount for each sale in the last 30 days.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Customers.customer_name,\n        Products.product_name,\n        Sales.total_amount\n    FROM\n        Sales\n    LEFT JOIN Customers\n        ON Sales.customer_id = Customers.customer_id\n    LEFT JOIN Products\n        ON Sales.product_id = Products.product_id\n    WHERE\n        Sales.sales_date &gt;= DATE('now', '-30 days');\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      product_name\n      total_amount\n    \n  \n  \n    \n      0\n      Jane Young\n      Laptop\n      3000\n    \n  \n\n\n\n\n\n\nQ2\nWrite a query to find the total revenue generated by each product category in the last year. The output should include the product category and the total revenue for that category.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Products.category,\n        SUM(Sales.total_amount) AS total_revenue\n    FROM\n        Sales\n    LEFT JOIN Products\n        ON Sales.product_id = Products.product_id\n    WHERE\n        Sales.sales_date &gt;= DATE('now', '-1 year')\n    GROUP BY\n        category;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      category\n      total_revenue\n    \n  \n  \n    \n      0\n      Electronics\n      11000\n    \n  \n\n\n\n\n\n\nQ3\nWrite a query to return all customers who made purchases in 2023 and are located in the “West” region.\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT\n        Customers.customer_name\n    FROM\n        Customers\n    INNER JOIN Sales\n        ON Customers.customer_id = Sales.customer_id\n    WHERE\n        strftime('%Y', Sales.sales_date) = '2023'\n        AND Customers.sales_region = 'West';\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n    \n  \n  \n    \n      0\n      John Doe\n    \n  \n\n\n\n\n\n\nQ4\nWrite a query to display the total number of sales, total quantity sold, and total revenue for each customer. The result should include the customer_name, total sales, total quantity, and total revenue.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Customers.customer_name,\n        COUNT(Sales.sales_id) AS total_sales,\n        SUM(Sales.quantity) AS total_quantity,\n        SUM(Sales.total_amount) AS total_revenue\n    FROM\n        Customers\n    LEFT JOIN Sales\n        ON Sales.customer_id = Customers.customer_id\n    GROUP BY\n        Customers.customer_id;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      total_sales\n      total_quantity\n      total_revenue\n    \n  \n  \n    \n      0\n      John Doe\n      2\n      42\n      43000\n    \n    \n      1\n      Jane Young\n      2\n      7\n      9000\n    \n    \n      2\n      Chris Nguyen\n      1\n      9\n      8000\n    \n  \n\n\n\n\n\n\nQ5\nWrite a query to find the top 3 customers (by total revenue) in the year 2023.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Customers.customer_name,\n        SUM(Sales.total_amount) AS total_revenue\n    FROM\n        Customers\n    LEFT JOIN Sales\n        ON Customers.customer_id = Sales.customer_id\n        AND strftime('%Y', Sales.sales_date) = '2023'\n    GROUP BY\n        Customers.customer_name\n    ORDER BY\n        total_revenue DESC;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      total_revenue\n    \n  \n  \n    \n      0\n      John Doe\n      43000.0\n    \n    \n      1\n      Jane Young\n      6000.0\n    \n    \n      2\n      Chris Nguyen\n      NaN\n    \n  \n\n\n\n\n\n\nQ6\nWrite a query to rank products by their total sales quantity in 2023. The result should include the product_name, total quantity sold, and rank.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Products.product_name,\n        SUM(Sales.quantity) AS total_quantity,\n        RANK() OVER\n            (ORDER BY SUM(Sales.quantity) DESC) AS quantity_rank\n    FROM\n        Products\n    LEFT JOIN Sales\n        ON Products.product_id = Sales.product_id\n        AND strftime('%Y', Sales.sales_date) = '2023'\n    GROUP BY\n        Products.product_name;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      product_name\n      total_quantity\n      quantity_rank\n    \n  \n  \n    \n      0\n      Laptop\n      40.0\n      1\n    \n    \n      1\n      Washing machine\n      6.0\n      2\n    \n    \n      2\n      Phone\n      NaN\n      3\n    \n  \n\n\n\n\n\n\nQ7\nWrite a query that categorizes customers into “New” (if they signed up in the last 6 months) or “Existing” based on their sign_up_date. Include the customer_name, region, and category in the result.\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT\n        Customers.customer_name,\n        Customers.sales_region,\n    CASE\n        WHEN\n            Customers.sign_up_date &gt;= DATE('now', '-6 months')\n        THEN\n            'New'\n        ELSE\n            'Existing'\n    END AS customer_status\n    FROM\n        Customers\n    LEFT JOIN Sales\n        ON Customers.customer_id = Sales.customer_id;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      sales_region\n      customer_status\n    \n  \n  \n    \n      0\n      John Doe\n      West\n      Existing\n    \n    \n      1\n      Jane Young\n      South\n      New\n    \n    \n      2\n      Chris Nguyen\n      West\n      New\n    \n  \n\n\n\n\n\n\nQ8\nWrite a query to return the month and year along with the total sales for each month for the last 12 months.\n\n\nCreate a date dimension table\n# Create the table\nquery_create_table = \"\"\"\n    CREATE TABLE IF NOT EXISTS date_dim (\n        year        INTEGER,\n        month       INTEGER,\n        month_name  VARCHAR(255)\n    );\n\"\"\"\nconn.execute(query_create_table)\n\n# Create a date range\nstart_date = '2023-01-01'\nend_date = pd.to_datetime('now')\ndate_range = pd.date_range(\n    start=start_date, end=end_date, freq=\"ME\"\n)\n\n# Extract year and month pairs from the date range\nvalues = [\n    (date.year, date.month, date.month_name())\n    for date in date_range\n]\n\n# Insert the values into the table\nconn.executemany(\n    \"INSERT INTO date_dim (year, month, month_name) VALUES (?, ?, ?);\",\n    values\n)\n\n\n&lt;sqlite3.Cursor at 0x1178efe40&gt;\n\n\n\n\nCode\nquery = \"\"\"\n    SELECT\n        d.year AS sales_year,\n        d.month_name AS sales_month,\n        COALESCE(COUNT(S.sales_id), 0) AS total_sales\n    FROM\n        date_dim d\n    LEFT JOIN\n        Sales S ON d.year = strftime('%Y', S.sales_date)\n        AND d.month = strftime('%m', S.sales_date)\n    WHERE\n        d.year &gt;= strftime('%Y', DATE('now', '-12 months'))\n    GROUP BY\n        d.year, d.month\n    ORDER BY\n        d.year, d.month;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      sales_year\n      sales_month\n      total_sales\n    \n  \n  \n    \n      0\n      2023\n      January\n      1\n    \n    \n      1\n      2023\n      February\n      0\n    \n    \n      2\n      2023\n      March\n      0\n    \n    \n      3\n      2023\n      April\n      0\n    \n    \n      4\n      2023\n      May\n      0\n    \n    \n      5\n      2023\n      June\n      0\n    \n    \n      6\n      2023\n      July\n      0\n    \n    \n      7\n      2023\n      August\n      0\n    \n    \n      8\n      2023\n      September\n      2\n    \n    \n      9\n      2023\n      October\n      0\n    \n    \n      10\n      2023\n      November\n      0\n    \n    \n      11\n      2023\n      December\n      0\n    \n    \n      12\n      2024\n      January\n      0\n    \n    \n      13\n      2024\n      February\n      0\n    \n    \n      14\n      2024\n      March\n      0\n    \n    \n      15\n      2024\n      April\n      0\n    \n    \n      16\n      2024\n      May\n      0\n    \n    \n      17\n      2024\n      June\n      0\n    \n    \n      18\n      2024\n      July\n      0\n    \n    \n      19\n      2024\n      August\n      1\n    \n    \n      20\n      2024\n      September\n      1\n    \n  \n\n\n\n\n\n\nQ9\nWrite a query to return the product categories that generated more than $50,000 in revenue during the last 6 months.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Products.category,\n        SUM(Sales.total_amount) as total_revenue\n    FROM\n        Products\n    LEFT JOIN Sales\n        ON Products.product_id = Sales.product_id\n    WHERE\n        Sales.sales_date &gt;= DATE('now', '-25 months')\n    GROUP BY\n        Products.category\n    HAVING\n        SUM(Sales.total_amount) &gt;= 50000;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      category\n      total_revenue\n    \n  \n  \n    \n      0\n      Electronics\n      51000\n    \n  \n\n\n\n\n\n\nQ10\nWrite a query to check for any sales where the total_amount doesn’t match the expected value (i.e., quantity * price).\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Sales.*,\n        Products.price\n    FROM\n        Sales\n    LEFT JOIN Products\n        ON Sales.product_id = Products.product_id\n    WHERE\n        Sales.total_amount != Sales.quantity * Products.price\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      sales_id\n      customer_id\n      product_id\n      sales_date\n      quantity\n      total_amount\n      price\n    \n  \n  \n    \n      0\n      4\n      3\n      3\n      2024-08-22\n      9\n      8000\n      800"
  },
  {
    "objectID": "notebooks/sql_qs.html#wrap-up",
    "href": "notebooks/sql_qs.html#wrap-up",
    "title": "Exploring a database with SQL",
    "section": "Wrap up",
    "text": "Wrap up\n\n\nClose the database connection\ndb.save()\ndb.close()\n\n\nAnd that concludes this brief tour of using SQL to define, manipulate, and query tables in a database. In summary, we:\n\nused sqlite3 in Python to create a test database;\ndefined some tables;\ninserted values into those tables;\nran various queries on the tables;\nsaw key elements of SQL logic including grouping, filtering, ordering, joins, and datetime manipulation."
  },
  {
    "objectID": "notebooks/report.html",
    "href": "notebooks/report.html",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "",
    "text": "[…]"
  },
  {
    "objectID": "notebooks/report.html#imports",
    "href": "notebooks/report.html#imports",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Imports",
    "text": "Imports\n\n\nCode\nimport holidays\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport geopandas as gpd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\nimport arviz as az\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom great_tables import GT, nanoplot_options\nfrom itertools import combinations\nfrom pandas.plotting import scatter_matrix\nfrom IPython.display import (\n    display as display3,\n    Markdown\n)\n\nfrom src.paths import get_path_to\nfrom src.stylesheet import customize_plots\nfrom src.inspection import make_df, display, display2"
  },
  {
    "objectID": "notebooks/report.html#the-dataset",
    "href": "notebooks/report.html#the-dataset",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "The dataset",
    "text": "The dataset\n\nLoad the data\nWe begin by exploring the data to get to know the features and patterns on which we will base our analysis.\n\n\nLoad the data\nif 'data' not in locals():\n    data = pd.read_csv(\n        get_path_to(\"data\", \"raw\", \"PBJ_Daily_Nurse_Staffing_Q1_2024.zip\"),\n        encoding='ISO-8859-1',\n        low_memory=False\n    )\nelse:\n    print(\"data loaded.\")\n\n\n\n\nInspect the data\n\n\nCode\nGT(data.sample(5))\n\n\n\n\n\n\n\n\n  PROVNUM\n  PROVNAME\n  CITY\n  STATE\n  COUNTY_NAME\n  COUNTY_FIPS\n  CY_Qtr\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n\n\n\n  \n    115600\n    PRUITTHEALTH - LANIER\n    BUFORD\n    GA\n    Gwinnett\n    135\n    2024Q1\n    2024-02-11\n    86\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.75\n    34.75\n    0.0\n    1.0\n    1.0\n    0.0\n    51.0\n    51.0\n    0.0\n    97.0\n    97.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015211\n    TWIN OAKS REHABILITATION AND HEALTHCARE CENTER\n    MOBILE\n    AL\n    Mobile\n    97\n    2024Q1\n    2024-02-28\n    124\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    80.39\n    80.39\n    0.0\n    0.0\n    0.0\n    0.0\n    153.75\n    153.75\n    0.0\n    265.21\n    265.21\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    495282\n    LOUISA HEALTH & REHABILITATION CENTER\n    LOUISA\n    VA\n    Louisa\n    109\n    2024Q1\n    2024-03-11\n    88\n    9.5\n    0.0\n    9.5\n    13.0\n    13.0\n    0.0\n    4.0\n    4.0\n    0.0\n    24.0\n    24.0\n    0.0\n    77.75\n    77.75\n    0.0\n    129.0\n    129.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    345225\n    SIGNATURE HEALTHCARE OF CHAPEL HILL\n    CHAPEL HILL\n    NC\n    Orange\n    135\n    2024Q1\n    2024-01-14\n    97\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.33\n    29.26\n    5.07\n    0.0\n    0.0\n    0.0\n    50.86\n    43.63\n    7.23\n    185.74\n    185.74\n    0.0\n    0.0\n    0.0\n    0.0\n    11.99\n    11.99\n    0.0\n  \n  \n    555530\n    CHOWCHILLA MEMORIAL HEALTHCARE DISTRICT\n    CHOWCHILLA\n    CA\n    Madera\n    39\n    2024Q1\n    2024-03-23\n    29\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    7.98\n    7.98\n    0.0\n    0.0\n    0.0\n    0.0\n    23.85\n    23.85\n    0.0\n    73.2\n    73.2\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n\n\n\n\n\n\n        \n\n\n\n\nCode\ndf = data.describe().round(1)\nGT(df.reset_index())\n\n\n\n\n\n\n\n\n  index\n  COUNTY_FIPS\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n\n\n\n  \n    count\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n  \n  \n    mean\n    91.1\n    20240215.7\n    83.4\n    5.2\n    5.1\n    0.1\n    10.3\n    10.0\n    0.2\n    34.4\n    31.5\n    3.0\n    6.6\n    6.6\n    0.1\n    66.3\n    59.8\n    6.5\n    171.2\n    158.2\n    13.0\n    4.2\n    4.2\n    0.1\n    8.5\n    8.3\n    0.2\n  \n  \n    std\n    99.2\n    83.0\n    49.1\n    4.5\n    4.5\n    0.9\n    14.9\n    14.6\n    1.8\n    34.7\n    31.4\n    10.7\n    10.7\n    10.6\n    1.3\n    48.4\n    44.8\n    16.2\n    113.7\n    106.3\n    32.6\n    13.1\n    12.7\n    2.1\n    17.6\n    17.2\n    2.2\n  \n  \n    min\n    1.0\n    20240101.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    25%\n    31.0\n    20240123.0\n    51.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    13.0\n    12.0\n    0.0\n    0.0\n    0.0\n    0.0\n    32.8\n    28.2\n    0.0\n    97.0\n    88.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    50%\n    69.0\n    20240215.0\n    76.0\n    8.0\n    8.0\n    0.0\n    7.5\n    7.4\n    0.0\n    25.6\n    24.2\n    0.0\n    0.0\n    0.0\n    0.0\n    56.9\n    50.8\n    0.0\n    148.1\n    136.8\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    75%\n    117.0\n    20240309.0\n    104.0\n    8.0\n    8.0\n    0.0\n    16.0\n    16.0\n    0.0\n    44.8\n    41.5\n    0.0\n    9.0\n    8.8\n    0.0\n    88.6\n    81.2\n    5.8\n    217.0\n    203.1\n    11.0\n    0.0\n    0.0\n    0.0\n    11.2\n    10.8\n    0.0\n  \n  \n    max\n    840.0\n    20240331.0\n    743.0\n    327.8\n    327.8\n    42.0\n    266.2\n    266.2\n    92.5\n    908.6\n    904.2\n    430.6\n    246.8\n    246.8\n    154.4\n    614.6\n    604.0\n    454.0\n    1857.7\n    1573.1\n    694.3\n    452.0\n    279.0\n    280.5\n    395.6\n    395.6\n    128.9\n  \n\n\n\n\n\n\n        \n\n\n\n\nGroup the features\nWe note that there are 91 records per provider (len(data[\"WorkDate\"].unique())) and 1,330,966 records in the table overall. The following table, which collapses the raw data across providers, thus has 14,626 \\(\\left( \\frac{1330966}{91} \\right)\\) entries.\n\n\nCode\ndf = (\n    data.loc[:, [\n        \"STATE\",\n        \"COUNTY_NAME\", \"COUNTY_FIPS\",\n        \"CITY\",\n        \"PROVNAME\", \"PROVNUM\",\n    ]]\n    .value_counts()\n    .to_frame()\n    .rename(columns={0: 'Counts'})\n)\nGT(df.reset_index().head())\n\n\n\n\n\n\n\n\n\n\n\n  STATE\n  COUNTY_NAME\n  COUNTY_FIPS\n  CITY\n  PROVNAME\n  PROVNUM\n  Counts\n\n\n\n  \n    AK\n    Anchorage\n    20\n    ANCHORAGE\n    PRESTIGE CARE & REHAB CENTER OF ANCHORAGE\n    025025\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    LIMA CONVALESCENT HOME\n    366297\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    SHAWNEE MANOR\n    365361\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    SPRINGS OF LIMA THE\n    366464\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    SPRINGVIEW MANOR\n    366221\n    91\n  \n\n\n\n\n\n\n        \n\n\n\nTable 1: Record counts across state, country, city, and provider.\n\n\n\n\n\n\nCode\nGT(data[[\"CY_Qtr\", \"WorkDate\", \"MDScensus\"]].head())\n\n\n\n\n\n\n\n\n\n  CY_Qtr\n  WorkDate\n  MDScensus\n\n\n\n  \n    2024Q1\n    20240101\n    50\n  \n  \n    2024Q1\n    20240102\n    49\n  \n  \n    2024Q1\n    20240103\n    49\n  \n  \n    2024Q1\n    20240104\n    50\n  \n  \n    2024Q1\n    20240105\n    51\n  \n\n\n\n\n\n\n        \n\n\n\nClean the data\n\n\nCode\n# Normalize feature names (lowercase and remove underscores)\n\n# Normalize categorical features (make a column title case)\n# data['CITY'].str.title()"
  },
  {
    "objectID": "notebooks/report.html#explore-the-dataset",
    "href": "notebooks/report.html#explore-the-dataset",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Explore the dataset",
    "text": "Explore the dataset\n\nVisualize distributions\n\n\nVisualize relationships\n\n\nCode\nattributes = [\"Hrs_RN\", \"Hrs_LPN_ctr\", \"Hrs_CNA\", \"Hrs_NAtrn\", \"Hrs_MedAide\"]\nn = len(attributes)\n\nfig, axs = plt.subplots(n, n, figsize=(8, 8))\nscatter_matrix(\n    data[attributes].sample(200),\n    ax=axs, alpha=.7,\n    hist_kwds=dict(bins=15, linewidth=0)\n)\nfig.align_ylabels(axs[:, 0])\nfig.align_xlabels(axs[-1, :])\nfor ax in axs.flatten():\n    ax.tick_params(axis='both', which='both', length=3.5)\n\n# save_fig(\"scatter_matrix_plot\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Scatter matrix of nursing worker working hours\n\n\n\n\n\n\n\nCompare groups\n\n\n\n\n\n\nNote 1: [Recommendation].\n\n\n\n\n\n\n\n\nCode\nN_GROUPS = 6\nN_LEVELS = 1\n\ndata_ = np.random.normal(loc=5, scale=3.0, size=(N_GROUPS, N_LEVELS, 10))\n\n# Calculate averages and confidence intervals\naverages = np.mean(data_, axis=2)\nconf_intervals = np.zeros_like(averages, dtype=float)\n\nfor group_idx in range(N_GROUPS):\n    for level_idx in range(N_LEVELS):\n        interval = stats.t.interval(\n            0.95,\n            len(data_[group_idx, level_idx]) - 1,\n            loc=np.mean(data_[group_idx, level_idx]),\n            scale=stats.sem(data_[group_idx, level_idx])\n        )\n\n        # Use upper bound\n        conf_intervals[group_idx, level_idx] = np.abs(\n            interval[1] - averages[group_idx, level_idx]\n        )\n\n# -- Plot grouped bars with confidence intervals -----------------------------\n\nwidth = 0.2\ncolors = plt.cm.Blues_r(np.linspace(.15, .85, N_LEVELS))\nline_thickness = 0.6\nstagger_amount = 0.8\n\nfig, ax = plt.subplots()\n\nfor level_idx in range(N_LEVELS):\n    bars = ax.bar(\n        np.arange(N_GROUPS) + level_idx * width - (width * (N_LEVELS - 1) / 2),\n        averages[:, level_idx],\n        yerr=conf_intervals[:, level_idx],\n        width=width,\n        edgecolor=\"white\",\n        alpha=0.85,\n        # capsize=3,\n        color=colors[level_idx],\n        error_kw={'elinewidth': line_thickness, 'capsize': 0},\n        label=f'Level {level_idx + 1}',\n    )\n\n# Style\nax.set_ylabel('Values')\n\ngroup_labels = [f'Group {i}' for i in range(1, N_GROUPS + 1)]\nax.set_xticks(np.arange(N_GROUPS))\nax.set_xticklabels(group_labels, rotation=60, ha='right')\n\n# ax.legend(title='', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# -- Add staggered sigbars and asterisks for select btwn-group comparisons ---\n\nsignificance_level = 0.09\nstagger_index = 0\nstats_list = []\n\nfor comb in combinations(range(N_GROUPS), 2):\n    group1_center = ax.get_xticks()[comb[0]]\n    group2_center = ax.get_xticks()[comb[1]]\n\n    t_stat, p_value = stats.ttest_ind(\n        data_[comb[0], :, :].flatten(),\n        data_[comb[1], :, :].flatten()\n    )\n\n    if p_value &lt; significance_level:\n        tallest_bar_height = np.max(averages) + np.max(conf_intervals) + 0.5\n\n        # Adjust the stagger amount\n        significance_height = (\n            tallest_bar_height\n            + np.max(conf_intervals) * 0.07\n            + stagger_index * stagger_amount\n        )\n\n        # Plot staggered lines aligned with the midpoints of compared groups\n        ax.plot(\n            [group1_center, group2_center],\n            [significance_height] * 2,\n            color='black',\n            lw=line_thickness\n        )\n\n        # Plot asterisks aligned with the center of the significance bars\n        asterisks = (\n            '*' * sum([p_value &lt; alpha for alpha in [0.01, 0.001, 0.0001]])\n        )\n        ax.text(\n            (group1_center + group2_center) / 2,\n            significance_height,\n            asterisks,\n            ha='center',\n            va='bottom',\n            fontsize=10\n        )\n\n        # Increment the index for staggered bars\n        stagger_index += 1\n\n        # Store significant comparisons, t values, and sample sizes\n        sample_size1 = len(data_[comb[0], :, :].flatten())\n        sample_size2 = len(data_[comb[1], :, :].flatten())\n        stats_list.append({\n            \"Comparison\":\n                f'{group_labels[comb[0]]} vs {group_labels[comb[1]]}',\n            \"p-value\":\n                f\"{p_value:.4f}\",\n            \"t-statistic\":\n                f\"{t_stat:.4f}\",\n            \"Sample Size\": (\n                f'{group_labels[comb[0]]} = {sample_size1}, '\n                f'{group_labels[comb[1]]} = {sample_size2}'\n            )\n        })\n\n# Style and show\nax.spines[['top', 'right']].set_visible(False)\nax.spines[['bottom', 'left']].set_visible(False)\nax.set_axisbelow(True)\n\nax.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\nstats_df = pd.DataFrame(stats_list)\n\n\n\n\n\n\n\n\nFigure 2: Comparison of average nurse working hours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n\n\n\n\nTable 2: Results of group comparisons by independent t-tests.\n\n\n\n\n\nCode\ndf = data.copy()\ndf = df.melt(\n    value_vars=[\"Hrs_RN\", \"Hrs_LPN\", \"Hrs_CNA\", \"Hrs_NAtrn\", \"Hrs_MedAide\"],\n    var_name=\"Role\",\n    value_name=\"Hours\"\n)\n\n# Set up the figure and axes for subplots\nnum_categories = len(df[\"Role\"].unique())\nfig, axes = plt.subplots(num_categories, 1, figsize=(6, 3), sharex=True)\n\n# Sort categories to ensure consistent order\ncategories = sorted(df[\"Role\"].unique())\n\n# Plot each category's empirical distribution in its own row\nfor i, category in enumerate(categories):\n    ax = axes[i]\n    values = df[df[\"Role\"] == category][\"Hours\"].values\n\n    # Compute HDI\n    hdi = az.hdi(np.array(values), hdi_prob=0.94)\n    \n    # Plot KDE and HDI\n    sns.kdeplot(values, ax=ax, fill=True, alpha=0.7)\n    # ax.plot([hdi[0], hdi[1]], [0, 0], color=\"black\", lw=5)\n    \n    ax.set_ylabel(\n        category, rotation=0, labelpad=20, va=\"center\", ha=\"right\"\n    )\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_linewidth(.5)\n\n    # ax.tick_params(axis='x', which='major', length=1.5)\n    ax.yaxis.set_ticks([])\n    \nplt.xlim(-50, 600)\nplt.xlabel(\"Hours\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Distributions of nurse working hours."
  },
  {
    "objectID": "notebooks/report.html#feature-engineer",
    "href": "notebooks/report.html#feature-engineer",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Feature engineer",
    "text": "Feature engineer\n\nJoin geographical data\n\n\nLoad the data\nif 'uscities' not in locals():\n    uscities = pd.read_csv(\n        get_path_to(\"data\", \"raw\", \"uscities\", \"uscities.csv\"),\n        encoding='ISO-8859-1',\n        low_memory=False\n    )\nelse:\n    print(\"data loaded.\")\n    \nGT(uscities.sample(5))\n\n\n\n\nPrepare the dataset to be joined\n# Prepare text fields of `data`\ndata_ = data.copy()\ndata_[\"CITY\"] = data_[\"CITY\"].str.strip().str.title()\ndata_[\"STATE\"] = data_[\"STATE\"].str.strip().str.upper()\ndata_.rename(columns={\"STATE\": \"state\", \"CITY\": \"city\"}, inplace=True)\ndisplay3(GT(data_.head()))\n\n# Prepare text fields of `uscities`\nuscities['city'] = uscities['city'].str.strip().str.title()\nuscities['state_id'] = uscities['state_id'].str.strip().str.upper()\ndisplay3(GT(uscities.drop(columns=[\"zips\"]).head()))\n\n\n\n\n\n\n\n\n  PROVNUM\n  PROVNAME\n  city\n  state\n  COUNTY_NAME\n  COUNTY_FIPS\n  CY_Qtr\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n\n\n\n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240102\n    49\n    8.0\n    8.0\n    0.0\n    18.24\n    18.24\n    0.0\n    58.89\n    58.89\n    0.0\n    0.0\n    0.0\n    0.0\n    22.96\n    22.96\n    0.0\n    149.4\n    149.4\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240103\n    49\n    8.0\n    8.0\n    0.0\n    15.1\n    15.1\n    0.0\n    55.02\n    55.02\n    0.0\n    0.0\n    0.0\n    0.0\n    20.7\n    20.7\n    0.0\n    147.15\n    147.15\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240104\n    50\n    8.0\n    8.0\n    0.0\n    14.9\n    14.9\n    0.0\n    57.13\n    57.13\n    0.0\n    0.0\n    0.0\n    0.0\n    12.7\n    12.7\n    0.0\n    142.21\n    142.21\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240105\n    51\n    8.0\n    8.0\n    0.0\n    15.47\n    15.47\n    0.0\n    46.76\n    46.76\n    0.0\n    0.0\n    0.0\n    0.0\n    27.44\n    27.44\n    0.0\n    149.4\n    149.4\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  city\n  city_ascii\n  state_id\n  state_name\n  county_fips\n  county_name\n  lat\n  lng\n  population\n  density\n  source\n  military\n  incorporated\n  timezone\n  ranking\n  id\n\n\n\n  \n    New York\n    New York\n    NY\n    New York\n    36081\n    Queens\n    40.6943\n    -73.9249\n    18908608\n    11080.3\n    shape\n    False\n    True\n    America/New_York\n    1\n    1840034016\n  \n  \n    Los Angeles\n    Los Angeles\n    CA\n    California\n    6037\n    Los Angeles\n    34.1141\n    -118.4068\n    11922389\n    3184.7\n    shape\n    False\n    True\n    America/Los_Angeles\n    1\n    1840020491\n  \n  \n    Chicago\n    Chicago\n    IL\n    Illinois\n    17031\n    Cook\n    41.8375\n    -87.6866\n    8497759\n    4614.5\n    shape\n    False\n    True\n    America/Chicago\n    1\n    1840000494\n  \n  \n    Miami\n    Miami\n    FL\n    Florida\n    12086\n    Miami-Dade\n    25.784\n    -80.2101\n    6080145\n    4758.9\n    shape\n    False\n    True\n    America/New_York\n    1\n    1840015149\n  \n  \n    Houston\n    Houston\n    TX\n    Texas\n    48201\n    Harris\n    29.786\n    -95.3885\n    5970127\n    1384.0\n    shape\n    False\n    True\n    America/Chicago\n    1\n    1840020925\n  \n\n\n\n\n\n\n        \n\n\n\n\nJoin the datasets\n# Join\ndata_geo = data_.merge(\n    uscities[['city', 'lat', 'lng', 'population', 'state_id', 'state_name']],\n    how='left',\n    left_on=['city', 'state'],\n    right_on=['city', 'state_id']\n).drop(columns=['state_id'])\n\nGT(data_geo.head())\n\n\n\n\n\n\n\n\n  PROVNUM\n  PROVNAME\n  city\n  state\n  COUNTY_NAME\n  COUNTY_FIPS\n  CY_Qtr\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n  lat\n  lng\n  population\n  state_name\n\n\n\n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.5055\n    -87.7283\n    10767.0\n    Alabama\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240102\n    49\n    8.0\n    8.0\n    0.0\n    18.24\n    18.24\n    0.0\n    58.89\n    58.89\n    0.0\n    0.0\n    0.0\n    0.0\n    22.96\n    22.96\n    0.0\n    149.4\n    149.4\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.5055\n    -87.7283\n    10767.0\n    Alabama\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240103\n    49\n    8.0\n    8.0\n    0.0\n    15.1\n    15.1\n    0.0\n    55.02\n    55.02\n    0.0\n    0.0\n    0.0\n    0.0\n    20.7\n    20.7\n    0.0\n    147.15\n    147.15\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.5055\n    -87.7283\n    10767.0\n    Alabama\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240104\n    50\n    8.0\n    8.0\n    0.0\n    14.9\n    14.9\n    0.0\n    57.13\n    57.13\n    0.0\n    0.0\n    0.0\n    0.0\n    12.7\n    12.7\n    0.0\n    142.21\n    142.21\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.5055\n    -87.7283\n    10767.0\n    Alabama\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240105\n    51\n    8.0\n    8.0\n    0.0\n    15.47\n    15.47\n    0.0\n    46.76\n    46.76\n    0.0\n    0.0\n    0.0\n    0.0\n    27.44\n    27.44\n    0.0\n    149.4\n    149.4\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.5055\n    -87.7283\n    10767.0\n    Alabama\n  \n\n\n\n\n\n\n        \n\n\n\n\nGroup by city\n# Sum aggregate hours columns\ndata_geo[\"total hours\"] = (\n    data_geo\n    .filter(regex=r'^Hrs_[^_]+$', axis='columns')\n    .sum(axis=1)\n)\n\n# Group hours by city (collapse across date)\ndata_geo_ = (\n    data_geo\n    .dropna()\n    .groupby(by=[\"city\"], as_index=False)\n    .agg({\n        \"total hours\": \"sum\",\n        \"state_name\": \"first\",\n        \"lat\": \"first\",\n        \"lng\": \"first\",\n        \"population\": \"first\"\n    })\n    .rename(columns={\"total hours\": \"total_hours_sum\"})\n)\n\nGT(data_geo_.head())\n\n\n\n\n\n\n\n\n  city\n  total_hours_sum\n  state_name\n  lat\n  lng\n  population\n\n\n\n  \n    Abbeville\n    143843.65\n    Alabama\n    31.5664\n    -85.2528\n    2309.0\n  \n  \n    Abbotsford\n    18874.74\n    Wisconsin\n    44.9435\n    -90.3174\n    2186.0\n  \n  \n    Aberdeen\n    179853.83\n    Mississippi\n    33.8287\n    -88.5539\n    4972.0\n  \n  \n    Abilene\n    250997.54\n    Kansas\n    38.923\n    -97.2252\n    6489.0\n  \n  \n    Abingdon\n    63043.01\n    Virginia\n    36.709\n    -81.9713\n    8346.0\n  \n\n\n\n\n\n\n        \n\n\n\n\nCode\ngdf = gpd.GeoDataFrame(\n    data_geo_,\n    geometry=gpd.points_from_xy(data_geo_[\"lng\"], data_geo_[\"lat\"])\n)\n\n# Load a world map\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n\n# Plot the map in the background\nfig, ax = plt.subplots(figsize=(8, 6))\nworld.plot(ax=ax, color=\"white\")\n\n# ?Normalize the hours for color mapping\nnorm = plt.Normalize(\n    vmin=gdf[\"total_hours_sum\"].min(),\n    vmax=gdf[\"total_hours_sum\"].max()\n)\ncmap = plt.cm.jet\n\n# Plot the cities on top of the US map, color and size by total_hours_sum\ngdf.plot(\n    ax=ax,\n    # ?\n    color=gdf[\"total_hours_sum\"].apply(lambda x: cmap(norm(x))),\n    # markersize=gdf[\"total_hours_sum\"] / 200000,\n    markersize=gdf[\"population\"] / 200000,\n    alpha=0.6\n)\n\n# Add labels to top 5 cities by total hours\ntop_cities = gdf.nlargest(5, \"total_hours_sum\")\nfor x, y, label in zip(\n    top_cities.geometry.x, top_cities.geometry.y, top_cities[\"city\"]\n):\n    ax.text(x, y, label, fontsize=8, fontweight=200, ha=\"right\")\n\n# Colorbar\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\ncbar = plt.colorbar(sm, ax=ax, shrink=0.5)\ncbar.ax.tick_params(labelsize=8)\ncbar.set_label(\"Total Hours\", fontsize=8)\n\n# Focus on the US longitude/latitude range\nplt.xlim([-130, -65])\nplt.ylim([20, 50])\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Hours worked by US city, represented by point size and colour.\n\n\n\n\n\n\n\nCode\n##| label: fig-geo2\n##| fig-cap: Hours worked by US city, represented by point size and colour.\n\n# pio.renderers.default = \"plotly_mimetype+notebook_connected\"\npio.renderers.default = \"notebook\"\n\n# Plotly Express scatter_geo plot\nfig = px.scatter_geo(\n    data_geo_,\n    lon='lng',\n    lat='lat',\n    # text='city',\n    size='population',\n    color='total_hours_sum',\n    hover_name='city',\n    hover_data={'total_hours_sum': True, 'population': True},\n    color_continuous_scale='Jet',\n    projection='natural earth'\n)\n\n# Customize layout, focusing on USA\nfig.update_layout(\n    title='Hours worked by US city',\n    geo=dict(\n        scope='usa',\n        projection_type='albers usa', \n        showland=True,\n        landcolor='white',\n        subunitcolor=\"black\",\n        countrycolor=\"black\",\n        coastlinecolor=\"black\",\n        visible=True\n    ),\n    dragmode=False,\n)\n\nfig.show()\n\n\n\n\nJoin seasonal data\n\n\nJoin the datasets\ndef add_time_features(df):\n    df['is_weekend'] = df.index.weekday &gt;= 5\n    df['day_of_year'] = df.index.dayofyear\n    \n    us_holidays = holidays.US()\n    df['is_holiday'] = df.index.isin(us_holidays)\n    \n    # df['day_index'] = np.arange(len(df))\n    # return df[['day_of_year', 'day_index', 'is_weekend', 'is_holiday']]\n    return df\n\ndata__ = data.copy()\ndata__['WorkDate'] = pd.to_datetime(data__['WorkDate'], format='%Y%m%d')\ndata__.set_index('WorkDate', inplace=True)\n\nGT(add_time_features(data__).head())\n\n\n\n\n\n\n\n\n  PROVNUM\n  PROVNAME\n  CITY\n  STATE\n  COUNTY_NAME\n  COUNTY_FIPS\n  CY_Qtr\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n  is_weekend\n  day_of_year\n  is_holiday\n\n\n\n  \n    015009\n    BURNS NURSING HOME, INC.\n    RUSSELLVILLE\n    AL\n    Franklin\n    59\n    2024Q1\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    False\n    1\n    False\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    RUSSELLVILLE\n    AL\n    Franklin\n    59\n    2024Q1\n    49\n    8.0\n    8.0\n    0.0\n    18.24\n    18.24\n    0.0\n    58.89\n    58.89\n    0.0\n    0.0\n    0.0\n    0.0\n    22.96\n    22.96\n    0.0\n    149.4\n    149.4\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    False\n    2\n    False\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    RUSSELLVILLE\n    AL\n    Franklin\n    59\n    2024Q1\n    49\n    8.0\n    8.0\n    0.0\n    15.1\n    15.1\n    0.0\n    55.02\n    55.02\n    0.0\n    0.0\n    0.0\n    0.0\n    20.7\n    20.7\n    0.0\n    147.15\n    147.15\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    False\n    3\n    False\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    RUSSELLVILLE\n    AL\n    Franklin\n    59\n    2024Q1\n    50\n    8.0\n    8.0\n    0.0\n    14.9\n    14.9\n    0.0\n    57.13\n    57.13\n    0.0\n    0.0\n    0.0\n    0.0\n    12.7\n    12.7\n    0.0\n    142.21\n    142.21\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    False\n    4\n    False\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    RUSSELLVILLE\n    AL\n    Franklin\n    59\n    2024Q1\n    51\n    8.0\n    8.0\n    0.0\n    15.47\n    15.47\n    0.0\n    46.76\n    46.76\n    0.0\n    0.0\n    0.0\n    0.0\n    27.44\n    27.44\n    0.0\n    149.4\n    149.4\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    False\n    5\n    False"
  },
  {
    "objectID": "notebooks/report.html#analyze-geography",
    "href": "notebooks/report.html#analyze-geography",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Analyze geography",
    "text": "Analyze geography"
  },
  {
    "objectID": "notebooks/report.html#analyze-seasonality",
    "href": "notebooks/report.html#analyze-seasonality",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Analyze seasonality",
    "text": "Analyze seasonality\n\n\nGroup by state, pivot on date and sum hours\ndf = data.copy()\n\nhours_columns = [\n    'Hrs_RNDON', 'Hrs_RNadmin', 'Hrs_LPNadmin',\n    'Hrs_CNA', 'Hrs_NAtrn', 'Hrs_MedAide'\n]\n\n# Sum hours across positions\ndf['Total_Hours'] = df[hours_columns].sum(axis=1)\n\n# Create a list of total hours per state over the work dates\ncity_hours = (\n    df.groupby(['STATE', 'WorkDate'])['Total_Hours']\n    .sum()\n    .reset_index()\n)\n\n# Pivot to create lists of total hours for each state\npivoted_city_hours = city_hours.pivot_table(\n    index=['STATE'],\n    columns='WorkDate',\n    values='Total_Hours',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Create a new column with lists of total hours over the 91 days\npivoted_city_hours['timeline'] = pivoted_city_hours.apply(\n    lambda row: {'val': row.tolist()}, axis=1\n)\npivoted_city_hours['avg'] = pivoted_city_hours.iloc[:, :-1].apply(\n    lambda row: row.mean(), axis=1\n)\n\n# Prepare the DataFrame for gt\ngt_df = pivoted_city_hours.reset_index()[['STATE', 'avg', 'timeline']]\n# gt_df_lines = gt_df.drop(columns=['avg'])\n# gt_df_avg   = gt_df.drop(columns=['lines'])\n\n# Create a line plot for total hours trajectory by state\n# gt_df['bars'] = gt_df['lines']\n\ngt_df.head()\n\n\n\n\nCode\n# Plot sparklines of average work hours across 91 days by state\n(\n    # GT(gt_df.head(), rowname_col=\"STATE\")\n    GT(gt_df.sort_values(by=\"avg\", ascending=False).head(10))\n    .fmt_nanoplot(\n        columns=\"avg\",\n    )\n    .fmt_nanoplot(\n        columns=\"timeline\",\n        # plot_type=\"bar\",\n        # reference_line=\"mean\",\n        # reference_area=[\"min\", \"q1\"],\n        autoscale=True,\n        # expand_x=[5, 16],\n        # expand_y=[0, 2],\n        options=nanoplot_options(\n            data_point_radius=3,\n            data_point_stroke_color=\"gray\",\n            data_point_stroke_width=1,\n            # data_point_fill_color=\"white\",\n            data_line_type=\"curve\",\n            # data_line_stroke_color=\"brown\",\n            # data_line_stroke_width=2,\n            # data_area_fill_color=\"orange\",\n            # vertical_guide_stroke_color=\"green\",\n            # show_data_area=True,\n            show_data_line=False\n        )\n    )\n    .tab_header(\n        title=\"Nurse hours worked in the United States\",\n        subtitle=\"The top 5 busiest states\",\n    )\n    # .tab_stubhead(label=\"State\")\n    .cols_label(\n        STATE=\"State\",\n        avg=\"Daily average hours worked\",\n        timeline=\"Trajectory over 91 days\",\n    )\n)\n\n\n\n\n\n\n\n\n\n  \n    Nurse hours worked in the United States\n  \n  \n    The top 5 busiest states\n  \n\n  State\n  Daily average hours worked\n  Trajectory over 91 days\n\n\n\n  \n    CA\n    271K\n    287K70.0K241K273K278K281K278K244K240K274K283K283K284K280K246K244K276K282K284K284K281K247K244K276K285K286K285K282K248K244K279K286K287K283K281K249K244K276K284K286K285K282K249K242K277K283K281K284K281K247K243K273K284K285K286K283K247K244K279K286K287K287K282K248K245K279K286K286K286K282K248K240K279K285K286K286K282K245K242K277K285K285K285K281K246K244K278K284K286K286K280K247K237K\n  \n  \n    NY\n    228K\n    287K70.0K187K227K241K242K235K201K184K231K241K245K247K239K203K191K223K238K247K246K237K203K192K236K245K249K248K240K207K194K235K245K251K249K240K207K196K237K246K251K248K240K205K193K235K236K243K244K237K201K194K225K243K248K246K236K203K194K235K244K249K245K238K203K195K235K245K250K249K240K205K189K235K246K250K247K239K204K193K235K244K250K247K239K202K196K235K244K248K245K231K200K184K\n  \n  \n    TX\n    188K\n    287K70.0K166K196K201K202K199K159K156K197K202K203K202K198K161K156K186K193K201K203K201K161K157K198K203K203K204K199K160K157K201K205K205K203K199K160K156K199K205K206K205K199K160K155K197K204K201K201K198K159K156K198K204K204K202K197K158K154K197K203K204K203K198K158K156K199K204K206K203K198K158K151K198K203K203K200K194K156K153K197K205K205K203K199K158K155K199K205K206K203K196K157K148K\n  \n  \n    FL\n    187K\n    287K70.0K170K192K195K196K193K167K164K193K194K197K197K193K168K166K192K197K198K198K193K169K166K194K198K200K199K194K170K166K194K198K200K198K194K169K166K195K198K200K199K194K169K165K193K197K197K197K194K169K166K194K198K199K198K193K168K166K194K198K199K197K194K168K166K195K198K199K198K194K168K161K194K197K199K197K193K168K166K194K198K199K198K194K169K165K194K198K199K198K192K168K161K\n  \n  \n    OH\n    156K\n    287K70.0K135K162K166K167K163K134K132K164K166K168K168K164K134K132K162K166K168K168K158K133K133K164K167K169K169K164K137K134K164K168K170K169K164K135K133K165K168K169K168K163K134K131K162K167K165K166K161K131K131K162K167K167K166K161K132K131K161K166K166K166K161K132K130K161K166K167K167K162K134K128K163K167K168K167K162K133K130K161K167K168K168K164K134K132K163K167K168K167K160K132K127K\n  \n  \n    PA\n    153K\n    287K70.0K132K157K164K163K159K134K129K159K162K166K166K161K137K133K158K156K165K165K154K136K134K160K165K167K166K161K138K133K159K165K166K166K160K138K135K161K166K168K166K160K137K131K158K157K162K163K158K133K133K159K163K166K163K158K134K131K157K163K164K163K157K134K132K158K164K165K164K158K136K128K158K163K165K164K158K134K131K159K164K166K163K159K136K133K159K164K165K163K154K134K128K\n  \n  \n    IL\n    129K\n    287K70.0K111K135K139K139K135K112K109K134K136K140K140K129K110K106K132K136K139K139K134K112K110K130K135K139K139K135K113K110K134K138K140K139K136K113K110K135K139K141K139K136K112K108K133K138K137K137K134K111K109K134K138K140K138K135K111K108K133K137K139K138K134K111K108K133K138K140K140K135K112K106K135K139K141K139K136K111K108K135K140K142K140K136K113K110K135K138K141K139K133K112K106K\n  \n  \n    NJ\n    96.2K\n    287K70.0K76.7K96.1K101K102K100K83.8K76.6K97.9K101K102K104K101K84.7K78.7K98.1K98.2K103K105K99.6K83.9K79.4K99.0K104K105K105K102K85.9K79.6K99.4K104K105K105K102K85.5K81.1K100.0K104K105K105K102K85.2K78.6K98.8K98.8K102K104K102K83.8K80.3K97.2K104K106K105K102K84.4K79.1K98.7K104K105K105K103K84.8K79.5K99.7K104K106K105K102K85.1K77.7K99.5K103K105K105K101K84.5K78.8K98.6K104K106K106K102K85.2K79.5K100K104K105K104K98.9K83.1K74.8K\n  \n  \n    IN\n    89.5K\n    287K70.0K78.0K94.3K96.4K96.6K95.3K77.7K76.8K94.3K95.1K97.3K96.1K93.9K78.0K76.5K92.8K94.5K95.9K96.1K92.5K77.7K76.8K93.8K94.9K96.4K96.7K95.1K78.5K77.1K94.8K96.1K97.1K96.6K94.7K77.7K76.3K93.9K95.8K97.0K96.6K94.6K77.8K75.7K93.2K95.2K95.0K95.1K92.4K75.6K75.5K93.6K95.9K96.3K95.4K94.0K77.1K75.5K93.4K94.8K96.1K96.0K93.2K77.0K76.0K93.4K95.0K96.4K96.4K94.3K77.1K73.9K94.4K95.6K96.5K96.2K93.7K76.7K75.7K93.8K94.9K96.1K95.9K93.3K77.6K75.4K92.8K94.4K95.1K95.4K91.8K76.5K73.4K\n  \n  \n    MI\n    87.1K\n    287K70.0K74.1K91.1K94.7K94.3K91.5K75.0K72.8K91.5K93.5K94.1K95.0K90.7K72.0K72.3K87.3K93.0K94.9K94.6K91.2K75.5K73.2K91.6K92.6K95.8K95.7K93.4K76.2K74.2K92.3K94.6K96.6K95.2K92.7K75.4K74.1K92.7K95.7K96.4K96.2K92.6K75.8K72.9K90.6K94.3K93.3K92.5K90.9K73.8K73.1K90.5K94.6K95.4K94.2K90.9K74.0K72.2K90.9K93.5K93.8K93.5K90.3K73.1K71.2K89.5K93.9K95.1K94.5K90.7K73.5K70.0K90.1K93.6K94.1K93.5K91.1K73.0K71.3K89.7K94.3K95.1K94.4K89.2K74.2K72.7K90.5K93.5K94.0K93.1K89.0K73.8K70.0K\n  \n\n\n\n\n\n\n        \n\n\nFigure 5: Sparklines of average work hours across 91 days by state."
  },
  {
    "objectID": "notebooks/report.html#model",
    "href": "notebooks/report.html#model",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "notebooks/report.html#concluding-thoughts",
    "href": "notebooks/report.html#concluding-thoughts",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\n(see Note 1)"
  }
]