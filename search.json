[
  {
    "objectID": "notebooks/report.html",
    "href": "notebooks/report.html",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "",
    "text": "[…]"
  },
  {
    "objectID": "notebooks/report.html#imports",
    "href": "notebooks/report.html#imports",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Imports",
    "text": "Imports\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom great_tables import GT\nfrom itertools import combinations\nfrom pandas.plotting import scatter_matrix\nfrom IPython.display import (\n    display as display3,\n    Markdown\n)\n\nfrom src.paths import get_path_to\nfrom src.stylesheet import customize_plots\nfrom src.inspection import make_df, display, display2"
  },
  {
    "objectID": "notebooks/report.html#the-dataset",
    "href": "notebooks/report.html#the-dataset",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "The dataset",
    "text": "The dataset\n\nLoad the data\nWe begin by exploring the data to get to know the features and patterns on which we will base our analysis.\n\n\nLoad the data\nif 'data' not in locals():\n    data = pd.read_csv(\n        get_path_to(\"data\", \"raw\", \"PBJ_Daily_Nurse_Staffing_Q1_2024.zip\"),\n        encoding='ISO-8859-1',\n        low_memory=False\n    )\nelse:\n    print(\"data loaded.\")\n\n\n\n\nInspect the data\n\n\nCode\nGT(data.sample(10))\n\n\n\n\n\n\n\n\n  PROVNUM\n  PROVNAME\n  CITY\n  STATE\n  COUNTY_NAME\n  COUNTY_FIPS\n  CY_Qtr\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n\n\n\n  \n    395405\n    QUAKERTOWN CENTER\n    QUAKERTOWN\n    PA\n    Bucks\n    17\n    2024Q1\n    20240308\n    120\n    8.0\n    8.0\n    0.0\n    26.0\n    26.0\n    0.0\n    63.72\n    55.78\n    7.94\n    0.0\n    0.0\n    0.0\n    90.21\n    66.13\n    24.08\n    207.12\n    191.95\n    15.17\n    15.52\n    15.52\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    145021\n    MACOMB POST ACUTE CARE CENTER\n    MACOMB\n    IL\n    Mc Donough\n    109\n    2024Q1\n    20240207\n    54\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    9.0\n    9.0\n    0.0\n    0.0\n    0.0\n    0.0\n    43.17\n    34.75\n    8.42\n    122.25\n    122.25\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015303\n    WEST HILL HEALTH AND REHAB\n    BIRMINGHAM\n    AL\n    Jefferson\n    73\n    2024Q1\n    20240209\n    84\n    8.0\n    8.0\n    0.0\n    24.0\n    24.0\n    0.0\n    8.0\n    8.0\n    0.0\n    0.18\n    0.18\n    0.0\n    47.97\n    47.97\n    0.0\n    139.94\n    139.94\n    0.0\n    0.0\n    0.0\n    0.0\n    25.86\n    25.86\n    0.0\n  \n  \n    366419\n    LEGACY TWINSBURG\n    TWINSBURG\n    OH\n    Summit\n    153\n    2024Q1\n    20240215\n    78\n    16.0\n    16.0\n    0.0\n    8.0\n    8.0\n    0.0\n    44.75\n    29.0\n    15.75\n    17.0\n    17.0\n    0.0\n    42.0\n    34.0\n    8.0\n    173.75\n    131.5\n    42.25\n    0.0\n    0.0\n    0.0\n    25.5\n    25.5\n    0.0\n  \n  \n    53A002\n    AMIE HOLT CARE CENTER\n    BUFFALO\n    WY\n    Johnson\n    19\n    2024Q1\n    20240222\n    28\n    8.0\n    8.0\n    0.0\n    8.9\n    8.9\n    0.0\n    27.6\n    27.6\n    0.0\n    0.0\n    0.0\n    0.0\n    19.8\n    19.8\n    0.0\n    70.9\n    70.9\n    0.0\n    44.3\n    44.3\n    0.0\n    9.7\n    9.7\n    0.0\n  \n  \n    555071\n    SUNNYVIEW CARE CENTER\n    LOS ANGELES\n    CA\n    Los Angeles\n    37\n    2024Q1\n    20240227\n    90\n    8.0\n    8.0\n    0.0\n    0.0\n    0.0\n    0.0\n    14.7\n    14.7\n    0.0\n    8.12\n    8.12\n    0.0\n    73.0\n    73.0\n    0.0\n    216.7\n    216.7\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    015177\n    WOODHAVEN MANOR NURSING HOME\n    DEMOPOLIS\n    AL\n    Marengo\n    91\n    2024Q1\n    20240318\n    71\n    8.0\n    8.0\n    0.0\n    29.25\n    29.25\n    0.0\n    8.0\n    8.0\n    0.0\n    0.0\n    0.0\n    0.0\n    73.17\n    67.17\n    6.0\n    150.25\n    150.25\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    105385\n    ROYAL OAK NURSING CENTER\n    DADE CITY\n    FL\n    Pasco\n    101\n    2024Q1\n    20240204\n    112\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    40.67\n    39.5\n    1.17\n    0.0\n    0.0\n    0.0\n    78.0\n    62.5\n    15.5\n    231.75\n    231.75\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    155167\n    WESTMINSTER VILLAGE NORTH\n    INDIANAPOLIS\n    IN\n    Marion\n    97\n    2024Q1\n    20240203\n    118\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.0\n    34.0\n    0.0\n    0.0\n    0.0\n    0.0\n    137.6\n    137.6\n    0.0\n    314.6\n    314.6\n    0.0\n    0.0\n    0.0\n    0.0\n    37.8\n    37.8\n    0.0\n  \n  \n    145704\n    APOSTOLIC CHRISTIAN HOME\n    ROANOKE\n    IL\n    Woodford\n    203\n    2024Q1\n    20240112\n    47\n    8.0\n    8.0\n    0.0\n    12.2\n    12.2\n    0.0\n    32.49\n    32.49\n    0.0\n    0.0\n    0.0\n    0.0\n    21.87\n    21.87\n    0.0\n    107.04\n    102.54\n    4.5\n    3.98\n    3.98\n    0.0\n    0.0\n    0.0\n    0.0\n  \n\n\n\n\n\n\n        \n\n\n\n\nCode\ndf = data.describe().round(1)\nGT(df.reset_index())\n\n\n\n\n\n\n\n\n  index\n  COUNTY_FIPS\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n\n\n\n  \n    count\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n    1330966.0\n  \n  \n    mean\n    91.1\n    20240215.7\n    83.4\n    5.2\n    5.1\n    0.1\n    10.3\n    10.0\n    0.2\n    34.4\n    31.5\n    3.0\n    6.6\n    6.6\n    0.1\n    66.3\n    59.8\n    6.5\n    171.2\n    158.2\n    13.0\n    4.2\n    4.2\n    0.1\n    8.5\n    8.3\n    0.2\n  \n  \n    std\n    99.2\n    83.0\n    49.1\n    4.5\n    4.5\n    0.9\n    14.9\n    14.6\n    1.8\n    34.7\n    31.4\n    10.7\n    10.7\n    10.6\n    1.3\n    48.4\n    44.8\n    16.2\n    113.7\n    106.3\n    32.6\n    13.1\n    12.7\n    2.1\n    17.6\n    17.2\n    2.2\n  \n  \n    min\n    1.0\n    20240101.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    25%\n    31.0\n    20240123.0\n    51.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    13.0\n    12.0\n    0.0\n    0.0\n    0.0\n    0.0\n    32.8\n    28.2\n    0.0\n    97.0\n    88.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    50%\n    69.0\n    20240215.0\n    76.0\n    8.0\n    8.0\n    0.0\n    7.5\n    7.4\n    0.0\n    25.6\n    24.2\n    0.0\n    0.0\n    0.0\n    0.0\n    56.9\n    50.8\n    0.0\n    148.1\n    136.8\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n  \n  \n    75%\n    117.0\n    20240309.0\n    104.0\n    8.0\n    8.0\n    0.0\n    16.0\n    16.0\n    0.0\n    44.8\n    41.5\n    0.0\n    9.0\n    8.8\n    0.0\n    88.6\n    81.2\n    5.8\n    217.0\n    203.1\n    11.0\n    0.0\n    0.0\n    0.0\n    11.2\n    10.8\n    0.0\n  \n  \n    max\n    840.0\n    20240331.0\n    743.0\n    327.8\n    327.8\n    42.0\n    266.2\n    266.2\n    92.5\n    908.6\n    904.2\n    430.6\n    246.8\n    246.8\n    154.4\n    614.6\n    604.0\n    454.0\n    1857.7\n    1573.1\n    694.3\n    452.0\n    279.0\n    280.5\n    395.6\n    395.6\n    128.9\n  \n\n\n\n\n\n\n        \n\n\n\n\nGroup the features\nWe note that there are 91 records per provider (len(data[\"WorkDate\"].unique())) and 1,330,966 records in the table overall. The following table, which collapses the raw data across providers, thus has 14,626 \\(\\left( \\frac{1330966}{91} \\right)\\) entries.\n\n\nCode\ndf = (\n    data.loc[:, [\n        \"STATE\",\n        \"COUNTY_NAME\", \"COUNTY_FIPS\",\n        \"CITY\",\n        \"PROVNAME\", \"PROVNUM\",\n    ]]\n    .value_counts()\n    .to_frame()\n    .rename(columns={0: 'Counts'})\n)\nGT(df.reset_index().head(n=20))\n\n\n\n\n\n\n\n\n\n\n\n  STATE\n  COUNTY_NAME\n  COUNTY_FIPS\n  CITY\n  PROVNAME\n  PROVNUM\n  Counts\n\n\n\n  \n    AK\n    Anchorage\n    20\n    ANCHORAGE\n    PRESTIGE CARE & REHAB CENTER OF ANCHORAGE\n    025025\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    LIMA CONVALESCENT HOME\n    366297\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    SHAWNEE MANOR\n    365361\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    SPRINGS OF LIMA THE\n    366464\n    91\n  \n  \n    OH\n    Allen\n    3\n    LIMA\n    SPRINGVIEW MANOR\n    366221\n    91\n  \n  \n    OH\n    Allen\n    3\n    SPENCERVILLE\n    ROSELAWN MANOR\n    365744\n    91\n  \n  \n    OH\n    Ashland\n    5\n    ASHLAND\n    BRETHREN CARE VILLAGE HEALTH CARE CENTER\n    366166\n    91\n  \n  \n    OH\n    Ashland\n    5\n    ASHLAND\n    CRYSTAL CARE CENTER OF ASHLAND\n    366239\n    91\n  \n  \n    OH\n    Ashland\n    5\n    ASHLAND\n    GOOD SHEPHERD THE\n    365093\n    91\n  \n  \n    OH\n    Ashland\n    5\n    ASHLAND\n    KINGSTON OF ASHLAND\n    365646\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    ANDOVER\n    ANDOVER VILLAGE RETIREMENT COMMUNITY\n    365411\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    ASHTABULA\n    CARINGTON PARK\n    365286\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    ASHTABULA\n    COUNTRY CLUB RET CENTER I I I\n    365642\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    ASHTABULA\n    SAYBROOK LANDING\n    366382\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    AUSTINBURG\n    AUSTINBURG NSG AND  REHAB CTR\n    366088\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    CONNEAUT\n    LAKE POINTE REHABILITATION AND NURSING CENTER\n    365441\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    GENEVA\n    GENEVA CENTER FOR REHABILITATION AND NURSING\n    366326\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    GENEVA\n    PINE GROVE HEALTHCARE CENTER\n    366366\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    GENEVA\n    RAE ANN GENEVA\n    366047\n    91\n  \n  \n    OH\n    Ashtabula\n    7\n    JEFFERSON\n    JEFFERSON HEALTHCARE CENTER\n    365638\n    91\n  \n\n\n\n\n\n\n        \n\n\n\nTable 1: Record counts across state, country, city, and provider.\n\n\n\n\n\n\nCode\nGT(data[[\"CY_Qtr\", \"WorkDate\", \"MDScensus\"]].head())\n\n\n\n\n\n\n\n\n\n  CY_Qtr\n  WorkDate\n  MDScensus\n\n\n\n  \n    2024Q1\n    20240101\n    50\n  \n  \n    2024Q1\n    20240102\n    49\n  \n  \n    2024Q1\n    20240103\n    49\n  \n  \n    2024Q1\n    20240104\n    50\n  \n  \n    2024Q1\n    20240105\n    51\n  \n\n\n\n\n\n\n        \n\n\n\nCode\n# Normalize feature names\n# data.rename(columns=lambda name: name.replace('_', ' ').capitalize())\n\n# Group by location for geographic view\n# data.groupby(by=[\"STATE\", \"CITY\"])[\"Hrs_NAtrn\"].sum()\n\n# Pivot by date for historical view\n\n# Query\n# (data\n#  .query(\"Hrs_NAtrn &gt; 0 and STATE in ['AL', 'AZ'] and WorkDate &gt; 20240315\")\n# )\n\n# Subset feature groups\ndata.filter(regex=r'^Hrs_[^_]+$', axis='columns')\n# data.filter(like=\"Hrs_RNDON\", axis=\"columns\")\n\n# Drop\n# data.drop(columns=data.filter(regex=\"^CY_Qtr$\").columns, inplace=False)\n\n\n\n\n\n  \n    \n      \n      Hrs_RNDON\n      Hrs_RNadmin\n      Hrs_RN\n      Hrs_LPNadmin\n      Hrs_LPN\n      Hrs_CNA\n      Hrs_NAtrn\n      Hrs_MedAide\n    \n  \n  \n    \n      0\n      8.0\n      8.00\n      40.07\n      0.00\n      18.16\n      156.34\n      0.00\n      0.00\n    \n    \n      1\n      8.0\n      18.24\n      58.89\n      0.00\n      22.96\n      149.40\n      0.00\n      0.00\n    \n    \n      2\n      8.0\n      15.10\n      55.02\n      0.00\n      20.70\n      147.15\n      0.00\n      0.00\n    \n    \n      3\n      8.0\n      14.90\n      57.13\n      0.00\n      12.70\n      142.21\n      0.00\n      0.00\n    \n    \n      4\n      8.0\n      15.47\n      46.76\n      0.00\n      27.44\n      149.40\n      0.00\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1330961\n      8.0\n      4.33\n      13.33\n      8.00\n      100.22\n      139.88\n      15.34\n      22.70\n    \n    \n      1330962\n      8.0\n      0.00\n      8.68\n      7.00\n      117.53\n      158.07\n      16.26\n      17.47\n    \n    \n      1330963\n      8.0\n      0.00\n      12.70\n      4.25\n      76.10\n      146.04\n      5.57\n      17.94\n    \n    \n      1330964\n      0.0\n      5.48\n      29.19\n      0.00\n      73.43\n      115.43\n      0.00\n      14.35\n    \n    \n      1330965\n      0.0\n      0.00\n      16.54\n      0.00\n      69.37\n      96.48\n      9.63\n      14.62\n    \n  \n\n1330966 rows × 8 columns\n\n\n\n\n\nClean the data\n\n\nCode\n# Normalize feature names (lowercase and remove underscores)\n\n# Normalize categorical features (make a column title case)\n# data['CITY'].str.title()"
  },
  {
    "objectID": "notebooks/report.html#explore-the-dataset",
    "href": "notebooks/report.html#explore-the-dataset",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Explore the dataset",
    "text": "Explore the dataset\n\nVisualize distributions\n\n\nVisualize relationships\n\n\nCode\nattributes = [\"Hrs_RN\", \"Hrs_LPN_ctr\", \"Hrs_CNA\", \"Hrs_NAtrn\", \"Hrs_MedAide\"]\nn = len(attributes)\n\nfig, axs = plt.subplots(n, n, figsize=(8, 8))\nscatter_matrix(\n    data[attributes].sample(200),\n    ax=axs, alpha=.7,\n    hist_kwds=dict(bins=15, linewidth=0)\n)\nfig.align_ylabels(axs[:, 0])\nfig.align_xlabels(axs[-1, :])\nfor ax in axs.flatten():\n    ax.tick_params(axis='both', which='both', length=3.5)\n\n# save_fig(\"scatter_matrix_plot\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Scatter matrix of nursing worker working hours\n\n\n\n\n\n\n\nCompare groups\n\n\n\n\n\n\nNote 1: [Recommendation].\n\n\n\n\n\n\n\n\nCode\n##| fig-subcap:\n##|   - Average working hours with 95% confidence intervals.\n##|   - Results of group comparisons by independent t-tests.\n\nN_GROUPS = 6\nN_LEVELS = 1\n\ndata_ = np.random.normal(loc=5, scale=3.0, size=(N_GROUPS, N_LEVELS, 10))\n\n# Calculate averages and confidence intervals\naverages = np.mean(data_, axis=2)\nconf_intervals = np.zeros_like(averages, dtype=float)\n\nfor group_idx in range(N_GROUPS):\n    for level_idx in range(N_LEVELS):\n        interval = stats.t.interval(\n            0.95,\n            len(data_[group_idx, level_idx]) - 1,\n            loc=np.mean(data_[group_idx, level_idx]),\n            scale=stats.sem(data_[group_idx, level_idx])\n        )\n\n        # Use upper bound\n        conf_intervals[group_idx, level_idx] = np.abs(\n            interval[1] - averages[group_idx, level_idx]\n        )\n\n# -- Plot grouped bars with confidence intervals -----------------------------\n\nwidth = 0.2\ncolors = plt.cm.Blues_r(np.linspace(.15, .85, N_LEVELS))\nline_thickness = 0.6\nstagger_amount = 0.8\n\nfig, ax = plt.subplots()\n\nfor level_idx in range(N_LEVELS):\n    bars = ax.bar(\n        np.arange(N_GROUPS) + level_idx * width - (width * (N_LEVELS - 1) / 2),\n        averages[:, level_idx],\n        yerr=conf_intervals[:, level_idx],\n        width=width,\n        edgecolor=\"white\",\n        alpha=0.85,\n        # capsize=3,\n        color=colors[level_idx],\n        error_kw={'elinewidth': line_thickness, 'capsize': 0},\n        label=f'Level {level_idx + 1}',\n    )\n\n# Style\nax.set_ylabel('Values')\n\ngroup_labels = [f'Group {i}' for i in range(1, N_GROUPS + 1)]\nax.set_xticks(np.arange(N_GROUPS))\nax.set_xticklabels(group_labels, rotation=60, ha='right')\n\n# ax.legend(title='', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# -- Add staggered sigbars and asterisks for select btwn-group comparisons ---\n\nsignificance_level = 0.09\nstagger_index = 0\nstats_list = []\n\nfor comb in combinations(range(N_GROUPS), 2):\n    group1_center = ax.get_xticks()[comb[0]]\n    group2_center = ax.get_xticks()[comb[1]]\n\n    t_stat, p_value = stats.ttest_ind(\n        data_[comb[0], :, :].flatten(),\n        data_[comb[1], :, :].flatten()\n    )\n\n    if p_value &lt; significance_level:\n        tallest_bar_height = np.max(averages) + np.max(conf_intervals) + 0.5\n\n        # Adjust the stagger amount\n        significance_height = (\n            tallest_bar_height\n            + np.max(conf_intervals) * 0.07\n            + stagger_index * stagger_amount\n        )\n\n        # Plot staggered lines aligned with the midpoints of compared groups\n        ax.plot(\n            [group1_center, group2_center],\n            [significance_height] * 2,\n            color='black',\n            lw=line_thickness\n        )\n\n        # Plot asterisks aligned with the center of the significance bars\n        asterisks = (\n            '*' * sum([p_value &lt; alpha for alpha in [0.01, 0.001, 0.0001]])\n        )\n        ax.text(\n            (group1_center + group2_center) / 2,\n            significance_height,\n            asterisks,\n            ha='center',\n            va='bottom',\n            fontsize=10\n        )\n\n        # Increment the index for staggered bars\n        stagger_index += 1\n\n        # Store significant comparisons, t values, and sample sizes\n        sample_size1 = len(data_[comb[0], :, :].flatten())\n        sample_size2 = len(data_[comb[1], :, :].flatten())\n        stats_list.append({\n            \"Comparison\":\n                f'{group_labels[comb[0]]} vs {group_labels[comb[1]]}',\n            \"p-value\":\n                f\"{p_value:.4f}\",\n            \"t-statistic\":\n                f\"{t_stat:.4f}\",\n            \"Sample Size\": (\n                f'{group_labels[comb[0]]} = {sample_size1}, '\n                f'{group_labels[comb[1]]} = {sample_size2}'\n            )\n        })\n\n# Style and show\nax.spines[['top', 'right']].set_visible(False)\nax.spines[['bottom', 'left']].set_visible(False)\nax.set_axisbelow(True)\n\nax.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\nstats_df = pd.DataFrame(stats_list)\n\n\n\n\n\n\n\n\nFigure 2: Comparison of average nurse working hours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n\n\n\n\nTable 2: Results of group comparisons by independent t-tests."
  },
  {
    "objectID": "notebooks/report.html#feature-engineer",
    "href": "notebooks/report.html#feature-engineer",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Feature engineer",
    "text": "Feature engineer\n\nJoin geographical data\n\n\nLoad the data\nif 'uscities' not in locals():\n    uscities = pd.read_csv(\n        get_path_to(\"data\", \"raw\", \"uscities\", \"uscities.csv\"),\n        encoding='ISO-8859-1',\n        low_memory=False\n    )\nelse:\n    print(\"data loaded.\")\n    \nGT(uscities.sample(5))\n\n\n\n\nPrepare the dataset to be joined\ndata_ = data.copy()\ndata_[\"CITY\"] = data_['CITY'].str.title()\ndata_.rename(columns={\"CITY\": \"city\"}, inplace=True)\n\n\n\n# Join\ndata_geo = data_.join(uscities.set_index(\"city\")[[\"lat\", \"lng\"]], on=\"city\")\nGT(data_geo.head())\n\n\n\n\n\n\n\n  PROVNUM\n  PROVNAME\n  city\n  STATE\n  COUNTY_NAME\n  COUNTY_FIPS\n  CY_Qtr\n  WorkDate\n  MDScensus\n  Hrs_RNDON\n  Hrs_RNDON_emp\n  Hrs_RNDON_ctr\n  Hrs_RNadmin\n  Hrs_RNadmin_emp\n  Hrs_RNadmin_ctr\n  Hrs_RN\n  Hrs_RN_emp\n  Hrs_RN_ctr\n  Hrs_LPNadmin\n  Hrs_LPNadmin_emp\n  Hrs_LPNadmin_ctr\n  Hrs_LPN\n  Hrs_LPN_emp\n  Hrs_LPN_ctr\n  Hrs_CNA\n  Hrs_CNA_emp\n  Hrs_CNA_ctr\n  Hrs_NAtrn\n  Hrs_NAtrn_emp\n  Hrs_NAtrn_ctr\n  Hrs_MedAide\n  Hrs_MedAide_emp\n  Hrs_MedAide_ctr\n  lat\n  lng\n\n\n\n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    35.2762\n    -93.1383\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    34.5055\n    -87.7283\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    36.8394\n    -86.895\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    38.5127\n    -92.4385\n  \n  \n    015009\n    BURNS NURSING HOME, INC.\n    Russellville\n    AL\n    Franklin\n    59\n    2024Q1\n    20240101\n    50\n    8.0\n    8.0\n    0.0\n    8.0\n    8.0\n    0.0\n    40.07\n    40.07\n    0.0\n    0.0\n    0.0\n    0.0\n    18.16\n    18.16\n    0.0\n    156.34\n    156.34\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    0.0\n    36.258\n    -83.1966\n  \n\n\n\n\n\n\n        \n\n\n\n\nGroup by city\n# Sum aggregate hours columns\ndata_geo[\"total hours\"] = (\n    data_geo\n    .filter(regex=r'^Hrs_[^_]+$', axis='columns')\n    .sum(axis=1)\n)\n\n# Group hours by city (collapse across date)\ndata_geo_ = (\n    data_geo\n    .dropna()\n    .groupby(by=[\"city\"], as_index=False)\n    .agg({\"total hours\": \"sum\", \"lat\": \"first\", \"lng\": \"first\"})\n    .rename(columns={\"total hours\": \"total_hours_sum\"})\n)\n\ndata_geo_\n\n\n\n\n\n  \n    \n      \n      city\n      total_hours_sum\n      lat\n      lng\n    \n  \n  \n    \n      0\n      Abbeville\n      719218.25\n      29.9751\n      -92.1265\n    \n    \n      1\n      Abbotsford\n      18874.74\n      44.9435\n      -90.3174\n    \n    \n      2\n      Aberdeen\n      1438830.64\n      45.4649\n      -98.4686\n    \n    \n      3\n      Abilene\n      501995.08\n      32.4543\n      -99.7384\n    \n    \n      4\n      Abingdon\n      189129.03\n      36.7090\n      -81.9713\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4641\n      Zephyrhills\n      109127.03\n      28.2409\n      -82.1797\n    \n    \n      4642\n      Zion\n      277935.12\n      42.4603\n      -87.8511\n    \n    \n      4643\n      Zionsville\n      21331.67\n      39.9897\n      -86.3182\n    \n    \n      4644\n      Zumbrota\n      11981.41\n      44.2950\n      -92.6736\n    \n    \n      4645\n      Zwolle\n      11500.80\n      31.6398\n      -93.6412\n    \n  \n\n4646 rows × 4 columns\n\n\n\n\n\nCode\nimport geopandas as gpd\n\ngdf = gpd.GeoDataFrame(\n    data_geo_,\n    geometry=gpd.points_from_xy(data_geo_[\"lng\"], data_geo_[\"lat\"])\n)\n\n# Load a world map for plotting\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n# us = world[world['name'] == \"United States\"]\n\n# Plot the map in the background\nfig, ax = plt.subplots(figsize=(8, 6))\nworld.plot(ax=ax, color=\"white\")\n\n# ?Normalize the hours for color mapping\nnorm = plt.Normalize(\n    vmin=gdf[\"total_hours_sum\"].min(),\n    vmax=gdf[\"total_hours_sum\"].max()\n)\ncmap = plt.cm.Jet\n\n# Plot the cities on top of the US map, color and size by total_hours_sum\ngdf.plot(\n    ax=ax,\n    # ?\n    color=gdf[\"total_hours_sum\"].apply(lambda x: cmap(norm(x))),\n    markersize=gdf[\"total_hours_sum\"] / 500000,\n    alpha=0.6\n)\n\n# Add labels to cities with total_hours_sum &gt; 10,000\nfor x, y, label, hours in zip(\n    gdf.geometry.x, gdf.geometry.y, gdf[\"city\"], gdf[\"total_hours_sum\"]\n):\n    if hours &gt; 20000000:\n        ax.text(x, y, label, fontsize=8, ha=\"right\")\n\n# Add a color bar to show the scale of total_hours_sum\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])  # Only needed for colorbar\ncbar = plt.colorbar(sm, ax=ax, shrink=0.5)\n# cbar.set_label(\"Total Hours\")\n\nax.set_axis_off()\n\n# plt.title(\"US Cities with Total Nurse Hours\")\nplt.xlim([-130, -65])  # Focus on the US longitude range\nplt.ylim([20, 50])     # Focus on the US latitude range\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Hours worked by US city, represented by point size and colour.\n\n\n\n\n\n\n\nJoin seasonal data"
  },
  {
    "objectID": "notebooks/report.html#analyze-geography",
    "href": "notebooks/report.html#analyze-geography",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Analyze geography",
    "text": "Analyze geography"
  },
  {
    "objectID": "notebooks/report.html#analyze-seasonality",
    "href": "notebooks/report.html#analyze-seasonality",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Analyze seasonality",
    "text": "Analyze seasonality"
  },
  {
    "objectID": "notebooks/report.html#model",
    "href": "notebooks/report.html#model",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "notebooks/report.html#extra-visualizations",
    "href": "notebooks/report.html#extra-visualizations",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Extra visualizations",
    "text": "Extra visualizations\n\nSparklines\n\n\nCode\n# Plot sparklines of average work hours across 91 days by state\n(\n    GT(gt_df.head(), rowname_col=\"STATE\")\n    .fmt_nanoplot(\n        columns=\"lines\",\n        reference_line=\"mean\",\n        reference_area=[\"min\", \"q1\"]\n    )\n    .tab_header(\n        title=\"Nurse hours worked in the United States\",\n        subtitle=\"The top 5 busiest states\",\n    )\n    .tab_stubhead(label=\"State\")\n    .cols_label(\n        lines=\"Total hours worked over 91 days\",\n    )\n)\n\n\n\n\n\n\n\n\n\n  \n    Nurse hours worked in the United States\n  \n  \n    The top 5 busiest states\n  \n\n  State\n  Total hours worked over 91 days\n\n\n\n  \n    AK\n    2.71K3.00K2.12K2.21K2.71K2.76K2.84K2.75K2.36K2.24K2.89K2.92K2.87K2.87K2.86K2.33K2.21K2.77K2.88K2.95K2.96K2.86K2.24K2.12K2.76K2.91K2.95K2.93K2.87K2.36K2.20K2.81K2.89K2.95K3.00K2.84K2.39K2.31K2.74K2.93K2.91K2.94K2.84K2.42K2.21K2.70K2.87K2.88K2.83K2.76K2.39K2.25K2.70K2.92K2.92K2.92K2.83K2.48K2.32K2.81K2.89K3.00K2.91K2.86K2.46K2.38K2.90K2.92K2.98K2.92K2.91K2.51K2.39K2.85K2.94K2.80K2.87K2.84K2.46K2.38K2.91K2.91K2.98K2.97K2.89K2.47K2.37K2.88K2.88K2.89K2.90K2.80K2.36K2.24K\n  \n  \n    AL\n    56.9K63.5K44.3K48.1K58.7K61.4K61.4K58.9K48.1K47.4K57.9K59.2K62.6K61.9K59.3K48.2K47.8K58.7K56.2K59.3K60.2K59.0K47.9K46.9K59.6K61.6K62.5K62.2K60.0K48.0K47.7K60.4K62.6K63.4K62.0K60.3K48.5K47.7K60.9K63.1K63.3K62.8K60.5K48.4K46.8K58.7K61.3K60.9K61.5K59.2K47.1K46.6K59.3K62.0K62.7K61.9K59.2K46.9K46.4K60.1K62.1K62.7K62.7K58.6K46.8K46.6K60.3K62.1K62.8K62.7K60.0K47.9K45.4K59.6K62.0K63.1K62.4K60.0K46.8K46.3K59.9K62.7K63.5K62.8K59.9K48.0K47.0K59.6K61.9K63.4K62.0K58.3K46.4K44.3K\n  \n  \n    AR\n    48.1K54.0K37.7K42.3K51.4K53.1K53.0K50.6K41.0K39.9K50.9K52.9K53.5K53.1K51.3K40.9K39.3K46.2K49.2K52.0K51.4K51.4K41.0K39.5K47.9K52.2K53.1K53.0K51.8K40.2K39.1K50.8K52.5K53.5K53.0K51.5K40.6K39.4K51.0K52.8K53.2K52.5K51.2K39.7K38.3K50.0K52.7K52.2K51.5K50.3K39.8K38.0K49.9K51.9K52.3K51.5K50.2K38.9K38.5K49.9K52.1K53.1K51.9K50.6K39.8K38.1K50.5K52.3K53.3K51.9K50.5K40.0K38.0K50.6K52.7K53.5K52.4K50.8K40.0K39.0K50.4K52.6K52.7K52.0K49.6K39.6K38.7K50.3K53.1K54.0K53.5K51.5K39.9K37.7K\n  \n  \n    AZ\n    29.0K31.5K23.1K25.3K29.7K30.9K31.0K30.5K25.0K24.1K30.2K30.9K31.3K31.1K30.6K25.2K24.5K30.0K30.4K31.0K30.8K30.4K24.8K24.4K30.0K30.9K31.2K30.8K30.5K25.5K24.8K30.5K31.2K31.4K31.5K30.5K25.3K24.8K30.7K31.3K31.5K31.5K30.8K25.4K24.7K30.3K31.3K31.0K31.0K30.6K25.0K24.4K30.1K31.3K31.3K31.4K30.7K25.3K24.5K30.3K31.0K31.3K31.2K30.0K25.0K24.3K30.1K31.0K31.4K31.0K30.3K25.2K24.8K30.3K30.9K31.1K31.0K30.5K25.0K24.2K29.8K30.6K30.9K31.0K30.7K25.3K24.3K30.4K30.8K31.1K31.0K30.3K24.7K23.1K\n  \n  \n    CA\n    271K287K237K241K273K278K281K278K244K240K274K283K283K284K280K246K244K276K282K284K284K281K247K244K276K285K286K285K282K248K244K279K286K287K283K281K249K244K276K284K286K285K282K249K242K277K283K281K284K281K247K243K273K284K285K286K283K247K244K279K286K287K287K282K248K245K279K286K286K286K282K248K240K279K285K286K286K282K245K242K277K285K285K285K281K246K244K278K284K286K286K280K247K237K\n  \n\n\n\n\n\n\n        \n\n\nFigure 4: Sparklines of average work hours across 91 days by state."
  },
  {
    "objectID": "notebooks/report.html#concluding-thoughts",
    "href": "notebooks/report.html#concluding-thoughts",
    "title": "Nurse staffing strategies for enhanced patient care",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\n(see Note 1)"
  },
  {
    "objectID": "notebooks/sql_qs.html",
    "href": "notebooks/sql_qs.html",
    "title": "Exploring a database with SQL",
    "section": "",
    "text": "Code\nimport sqlite3\nimport pandas as pd\nfrom IPython.display import Markdown, display\n\nfrom src.util import DBManager"
  },
  {
    "objectID": "notebooks/sql_qs.html#imports",
    "href": "notebooks/sql_qs.html#imports",
    "title": "Exploring a database with SQL",
    "section": "",
    "text": "Code\nimport sqlite3\nimport pandas as pd\nfrom IPython.display import Markdown, display\n\nfrom src.util import DBManager"
  },
  {
    "objectID": "notebooks/sql_qs.html#define-the-tables",
    "href": "notebooks/sql_qs.html#define-the-tables",
    "title": "Exploring a database with SQL",
    "section": "Define the tables",
    "text": "Define the tables\nLet us imagine we have the following tables in our database:\n\nSales\n\n\nsales_id (INT)\ncustomer_id (INT)\nproduct_id (INT)\nsale_date (DATE)\nquantity (INT)\ntotal_amount (DECIMAL)\n\n\nCustomers\n\n\ncustomer_id (INT)\ncustomer_name (VARCHAR)\nsales_region (VARCHAR)\nsign_up_date (DATE)\n\n\nProducts\n\n\nproduct_id (INT)\nproduct_name (VARCHAR)\ncategory (VARCHAR)\nprice (DECIMAL)\n\nWe can simulate this scenario by creating a test database in Python, creating tables within it that match this description, and inserting some example values into the tables.\n\n\nCreate a database\ndb_name = \"testdatabase.db\"\ndb = DBManager(db_name)\nconn, cursor = db.open()\n\n\n\n\nCreate the tables\n# Create `Sales` table\ncursor.execute(\n    \"\"\"\n    CREATE TABLE IF NOT EXISTS Sales\n    (\n        sales_id        INTEGER PRIMARY KEY AUTOINCREMENT,\n        customer_id     INTEGER,\n        product_id      INTEGER,\n        sales_date      DATE,\n        quantity        INTEGER,\n        total_amount    DECIMAL(10, 2),\n        FOREIGN KEY (customer_id) REFERENCES Customers(customer_id),\n        FOREIGN KEY (product_id) REFERENCES Products(product_id)\n    );\n    \"\"\"\n)\n\n# Create `Customers` table\ncursor.execute(\n    \"\"\"\n    CREATE TABLE IF NOT EXISTS Customers\n    (\n        customer_id     INTEGER PRIMARY KEY AUTOINCREMENT,\n        customer_name   VARCHAR(255) NOT NULL,\n        sales_region    VARCHAR(255),\n        sign_up_date    DATE\n    );\n    \"\"\"\n)\n\n# Create `Products` table\ncursor.execute(\n    \"\"\"\n    CREATE TABLE IF NOT EXISTS Products\n    (\n        product_id      INTEGER PRIMARY KEY AUTOINCREMENT,\n        product_name    VARCHAR(255) NOT NULL,\n        category        VARCHAR(255),\n        price           DECIMAL(10, 2)\n    );\n    \"\"\"\n)\n\n\n&lt;sqlite3.Cursor at 0x1178efa40&gt;\n\n\n\n\nInsert example data into the tables\n# Insert record into `Customers` table\nquery = \"\"\"\n    INSERT INTO Customers\n    (customer_name, sales_region, sign_up_date)\n    VALUES (?, ?, ?);\n\"\"\"\nvalues = [\n    (\"John Doe\", \"West\", \"2023-09-25\"),\n    (\"Jane Young\", \"South\", \"2024-09-25\"),\n    (\"Chris Nguyen\", \"West\", \"2024-09-25\"),\n]\ncursor.executemany(query, values)\n\n# Insert record into `Products` table\nquery = \"\"\"\n    INSERT INTO Products\n    (product_name, category, price)\n    VALUES (?, ?, ?);\n\"\"\"\nvalues = [\n    (\"Washing machine\", \"Appliances\", 1500.00),\n    (\"Laptop\", \"Electronics\", 1000.00),\n    (\"Phone\", \"Electronics\", 800.00),\n]\ncursor.executemany(query, values)\n\n# Insert record into `Sales` table\nquery = \"\"\"\n    INSERT INTO Sales\n    (customer_id, product_id, sales_date, quantity, total_amount)\n    VALUES (?, ?, ?, ?, ?);\n\"\"\"\nvalues = [\n    (1, 1, \"2023-09-26\", 2, values[0][2] * 2),\n    (2, 1, \"2023-01-15\", 4, values[0][2] * 4),\n    (2, 2, \"2024-09-20\", 3, values[1][2] * 3),\n    (3, 3, \"2024-08-22\", 9, values[2][2] * 10),\n    (1, 2, \"2023-09-26\", 40, values[1][2] * 40),\n]\ncursor.executemany(query, values)\n\n\n&lt;sqlite3.Cursor at 0x1178efa40&gt;\n\n\n\n\nCheck the contents of the tables\n# Query `Sales`\nquery = \"\"\"\n    SELECT *\n    FROM Sales\n    LIMIT 5;\n\"\"\"\ndisplay(Markdown(\"**`Sales`**:\"), pd.read_sql(query, conn))\n\n# Query `Customers`\nquery = \"\"\"\n    SELECT *\n    FROM Customers\n    LIMIT 5;\n\"\"\"\ndisplay(Markdown(\"**`Customers`**:\"), pd.read_sql(query, conn))\n\n# Query `Products`\nquery = \"\"\"\n    SELECT *\n    FROM Products\n    LIMIT 5;\n\"\"\"\ndisplay(Markdown(\"**`Products`**:\"), pd.read_sql(query, conn))\n\n\nSales:\n\n\n\n\n\n  \n    \n      \n      sales_id\n      customer_id\n      product_id\n      sales_date\n      quantity\n      total_amount\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2023-09-26\n      2\n      3000\n    \n    \n      1\n      2\n      2\n      1\n      2023-01-15\n      4\n      6000\n    \n    \n      2\n      3\n      2\n      2\n      2024-09-20\n      3\n      3000\n    \n    \n      3\n      4\n      3\n      3\n      2024-08-22\n      9\n      8000\n    \n    \n      4\n      5\n      1\n      2\n      2023-09-26\n      40\n      40000\n    \n  \n\n\n\n\nCustomers:\n\n\n\n\n\n  \n    \n      \n      customer_id\n      customer_name\n      sales_region\n      sign_up_date\n    \n  \n  \n    \n      0\n      1\n      John Doe\n      West\n      2023-09-25\n    \n    \n      1\n      2\n      Jane Young\n      South\n      2024-09-25\n    \n    \n      2\n      3\n      Chris Nguyen\n      West\n      2024-09-25\n    \n  \n\n\n\n\nProducts:\n\n\n\n\n\n  \n    \n      \n      product_id\n      product_name\n      category\n      price\n    \n  \n  \n    \n      0\n      1\n      Washing machine\n      Appliances\n      1500\n    \n    \n      1\n      2\n      Laptop\n      Electronics\n      1000\n    \n    \n      2\n      3\n      Phone\n      Electronics\n      800"
  },
  {
    "objectID": "notebooks/sql_qs.html#query-the-tables",
    "href": "notebooks/sql_qs.html#query-the-tables",
    "title": "Exploring a database with SQL",
    "section": "Query the tables",
    "text": "Query the tables\nWith these example tables constructed, let us now run some queries.\n\nQ1\nWrite a query to return the customer_name, product_name, and total_amount for each sale in the last 30 days.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Customers.customer_name,\n        Products.product_name,\n        Sales.total_amount\n    FROM\n        Sales\n    LEFT JOIN Customers\n        ON Sales.customer_id = Customers.customer_id\n    LEFT JOIN Products\n        ON Sales.product_id = Products.product_id\n    WHERE\n        Sales.sales_date &gt;= DATE('now', '-30 days');\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      product_name\n      total_amount\n    \n  \n  \n    \n      0\n      Jane Young\n      Laptop\n      3000\n    \n  \n\n\n\n\n\n\nQ2\nWrite a query to find the total revenue generated by each product category in the last year. The output should include the product category and the total revenue for that category.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Products.category,\n        SUM(Sales.total_amount) AS total_revenue\n    FROM\n        Sales\n    LEFT JOIN Products\n        ON Sales.product_id = Products.product_id\n    WHERE\n        Sales.sales_date &gt;= DATE('now', '-1 year')\n    GROUP BY\n        category;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      category\n      total_revenue\n    \n  \n  \n    \n      0\n      Electronics\n      11000\n    \n  \n\n\n\n\n\n\nQ3\nWrite a query to return all customers who made purchases in 2023 and are located in the “West” region.\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT\n        Customers.customer_name\n    FROM\n        Customers\n    INNER JOIN Sales\n        ON Customers.customer_id = Sales.customer_id\n    WHERE\n        strftime('%Y', Sales.sales_date) = '2023'\n        AND Customers.sales_region = 'West';\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n    \n  \n  \n    \n      0\n      John Doe\n    \n  \n\n\n\n\n\n\nQ4\nWrite a query to display the total number of sales, total quantity sold, and total revenue for each customer. The result should include the customer_name, total sales, total quantity, and total revenue.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Customers.customer_name,\n        COUNT(Sales.sales_id) AS total_sales,\n        SUM(Sales.quantity) AS total_quantity,\n        SUM(Sales.total_amount) AS total_revenue\n    FROM\n        Customers\n    LEFT JOIN Sales\n        ON Sales.customer_id = Customers.customer_id\n    GROUP BY\n        Customers.customer_id;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      total_sales\n      total_quantity\n      total_revenue\n    \n  \n  \n    \n      0\n      John Doe\n      2\n      42\n      43000\n    \n    \n      1\n      Jane Young\n      2\n      7\n      9000\n    \n    \n      2\n      Chris Nguyen\n      1\n      9\n      8000\n    \n  \n\n\n\n\n\n\nQ5\nWrite a query to find the top 3 customers (by total revenue) in the year 2023.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Customers.customer_name,\n        SUM(Sales.total_amount) AS total_revenue\n    FROM\n        Customers\n    LEFT JOIN Sales\n        ON Customers.customer_id = Sales.customer_id\n        AND strftime('%Y', Sales.sales_date) = '2023'\n    GROUP BY\n        Customers.customer_name\n    ORDER BY\n        total_revenue DESC;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      total_revenue\n    \n  \n  \n    \n      0\n      John Doe\n      43000.0\n    \n    \n      1\n      Jane Young\n      6000.0\n    \n    \n      2\n      Chris Nguyen\n      NaN\n    \n  \n\n\n\n\n\n\nQ6\nWrite a query to rank products by their total sales quantity in 2023. The result should include the product_name, total quantity sold, and rank.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Products.product_name,\n        SUM(Sales.quantity) AS total_quantity,\n        RANK() OVER\n            (ORDER BY SUM(Sales.quantity) DESC) AS quantity_rank\n    FROM\n        Products\n    LEFT JOIN Sales\n        ON Products.product_id = Sales.product_id\n        AND strftime('%Y', Sales.sales_date) = '2023'\n    GROUP BY\n        Products.product_name;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      product_name\n      total_quantity\n      quantity_rank\n    \n  \n  \n    \n      0\n      Laptop\n      40.0\n      1\n    \n    \n      1\n      Washing machine\n      6.0\n      2\n    \n    \n      2\n      Phone\n      NaN\n      3\n    \n  \n\n\n\n\n\n\nQ7\nWrite a query that categorizes customers into “New” (if they signed up in the last 6 months) or “Existing” based on their sign_up_date. Include the customer_name, region, and category in the result.\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT\n        Customers.customer_name,\n        Customers.sales_region,\n    CASE\n        WHEN\n            Customers.sign_up_date &gt;= DATE('now', '-6 months')\n        THEN\n            'New'\n        ELSE\n            'Existing'\n    END AS customer_status\n    FROM\n        Customers\n    LEFT JOIN Sales\n        ON Customers.customer_id = Sales.customer_id;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      customer_name\n      sales_region\n      customer_status\n    \n  \n  \n    \n      0\n      John Doe\n      West\n      Existing\n    \n    \n      1\n      Jane Young\n      South\n      New\n    \n    \n      2\n      Chris Nguyen\n      West\n      New\n    \n  \n\n\n\n\n\n\nQ8\nWrite a query to return the month and year along with the total sales for each month for the last 12 months.\n\n\nCreate a date dimension table\n# Create the table\nquery_create_table = \"\"\"\n    CREATE TABLE IF NOT EXISTS date_dim (\n        year        INTEGER,\n        month       INTEGER,\n        month_name  VARCHAR(255)\n    );\n\"\"\"\nconn.execute(query_create_table)\n\n# Create a date range\nstart_date = '2023-01-01'\nend_date = pd.to_datetime('now')\ndate_range = pd.date_range(\n    start=start_date, end=end_date, freq=\"ME\"\n)\n\n# Extract year and month pairs from the date range\nvalues = [\n    (date.year, date.month, date.month_name())\n    for date in date_range\n]\n\n# Insert the values into the table\nconn.executemany(\n    \"INSERT INTO date_dim (year, month, month_name) VALUES (?, ?, ?);\",\n    values\n)\n\n\n&lt;sqlite3.Cursor at 0x1178efe40&gt;\n\n\n\n\nCode\nquery = \"\"\"\n    SELECT\n        d.year AS sales_year,\n        d.month_name AS sales_month,\n        COALESCE(COUNT(S.sales_id), 0) AS total_sales\n    FROM\n        date_dim d\n    LEFT JOIN\n        Sales S ON d.year = strftime('%Y', S.sales_date)\n        AND d.month = strftime('%m', S.sales_date)\n    WHERE\n        d.year &gt;= strftime('%Y', DATE('now', '-12 months'))\n    GROUP BY\n        d.year, d.month\n    ORDER BY\n        d.year, d.month;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      sales_year\n      sales_month\n      total_sales\n    \n  \n  \n    \n      0\n      2023\n      January\n      1\n    \n    \n      1\n      2023\n      February\n      0\n    \n    \n      2\n      2023\n      March\n      0\n    \n    \n      3\n      2023\n      April\n      0\n    \n    \n      4\n      2023\n      May\n      0\n    \n    \n      5\n      2023\n      June\n      0\n    \n    \n      6\n      2023\n      July\n      0\n    \n    \n      7\n      2023\n      August\n      0\n    \n    \n      8\n      2023\n      September\n      2\n    \n    \n      9\n      2023\n      October\n      0\n    \n    \n      10\n      2023\n      November\n      0\n    \n    \n      11\n      2023\n      December\n      0\n    \n    \n      12\n      2024\n      January\n      0\n    \n    \n      13\n      2024\n      February\n      0\n    \n    \n      14\n      2024\n      March\n      0\n    \n    \n      15\n      2024\n      April\n      0\n    \n    \n      16\n      2024\n      May\n      0\n    \n    \n      17\n      2024\n      June\n      0\n    \n    \n      18\n      2024\n      July\n      0\n    \n    \n      19\n      2024\n      August\n      1\n    \n    \n      20\n      2024\n      September\n      1\n    \n  \n\n\n\n\n\n\nQ9\nWrite a query to return the product categories that generated more than $50,000 in revenue during the last 6 months.\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Products.category,\n        SUM(Sales.total_amount) as total_revenue\n    FROM\n        Products\n    LEFT JOIN Sales\n        ON Products.product_id = Sales.product_id\n    WHERE\n        Sales.sales_date &gt;= DATE('now', '-25 months')\n    GROUP BY\n        Products.category\n    HAVING\n        SUM(Sales.total_amount) &gt;= 50000;\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      category\n      total_revenue\n    \n  \n  \n    \n      0\n      Electronics\n      51000\n    \n  \n\n\n\n\n\n\nQ10\nWrite a query to check for any sales where the total_amount doesn’t match the expected value (i.e., quantity * price).\n\n\nCode\nquery = \"\"\"\n    SELECT\n        Sales.*,\n        Products.price\n    FROM\n        Sales\n    LEFT JOIN Products\n        ON Sales.product_id = Products.product_id\n    WHERE\n        Sales.total_amount != Sales.quantity * Products.price\n\"\"\"\npd.read_sql(query, conn)\n\n\n\n\n\n  \n    \n      \n      sales_id\n      customer_id\n      product_id\n      sales_date\n      quantity\n      total_amount\n      price\n    \n  \n  \n    \n      0\n      4\n      3\n      3\n      2024-08-22\n      9\n      8000\n      800"
  },
  {
    "objectID": "notebooks/sql_qs.html#wrap-up",
    "href": "notebooks/sql_qs.html#wrap-up",
    "title": "Exploring a database with SQL",
    "section": "Wrap up",
    "text": "Wrap up\n\n\nClose the database connection\ndb.save()\ndb.close()\n\n\nAnd that concludes this brief tour of using SQL to define, manipulate, and query tables in a database. In summary, we:\n\nused sqlite3 in Python to create a test database;\ndefined some tables;\ninserted values into those tables;\nran various queries on the tables;\nsaw key elements of SQL logic including grouping, filtering, ordering, joins, and datetime manipulation."
  }
]